[{"authors":["admin"],"categories":null,"content":"Currently pursuing my masters in Computer Science from The University of Texas at Arlington having specialization in Artificial Intelligence and Database.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://sushantmhambrey.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Currently pursuing my masters in Computer Science from The University of Texas at Arlington having specialization in Artificial Intelligence and Database.","tags":null,"title":"Sushant Mhambrey","type":"authors"},{"authors":[],"categories":[],"content":"","date":1583489978,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583489978,"objectID":"5331ef7925d4e42203f8c3e4ab77783f","permalink":"https://sushantmhambrey.github.io/project/nlp/","publishdate":"2020-03-06T04:19:38-06:00","relpermalink":"/project/nlp/","section":"project","summary":"real or fake tweet detection from a handpicked dataset of 10000 tweets using the concepts of natural languauge processing.","tags":[],"title":"NLP","type":"project"},{"authors":[],"categories":[],"content":"","date":1580360449,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580360449,"objectID":"9785e6cbb2b1405c01e51871f2357d05","permalink":"https://sushantmhambrey.github.io/project/pipe/","publishdate":"2020-01-29T23:00:49-06:00","relpermalink":"/project/pipe/","section":"project","summary":"Implemented a simple data pipeline to count the number of visitors on a website each day","tags":[],"title":"Data Pipeline ","type":"project"},{"authors":[],"categories":[],"content":"","date":1580360439,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580360439,"objectID":"1e1448d2639e1b7b7d45f78ea9644207","permalink":"https://sushantmhambrey.github.io/project/be/","publishdate":"2020-01-29T23:00:39-06:00","relpermalink":"/project/be/","section":"project","summary":"Implemented a project to virtually try footwear through a mobile application. Created AR models for shoes using Vuforia and Unity3d. Used YOLO algorithm to train image dataset","tags":[],"title":"Virtual Shoe Trial","type":"project"},{"authors":[],"categories":[],"content":"","date":1580360432,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580360432,"objectID":"a6d8e05c0c4b41be700bae6f8cb99b68","permalink":"https://sushantmhambrey.github.io/project/damt/","publishdate":"2020-01-29T23:00:32-06:00","relpermalink":"/project/damt/","section":"project","summary":"Implemented a project based on the probability distribution and their distribution functions .Simulated and analyzed various discrete and continuous probability distributions using Python3.  ","tags":[],"title":"Simulation of Random Variables","type":"project"},{"authors":[],"categories":[],"content":"","date":1580358031,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580358031,"objectID":"957a80445972d2e6a8474d2b53cd2a01","permalink":"https://sushantmhambrey.github.io/project/ai/","publishdate":"2020-01-29T22:20:31-06:00","relpermalink":"/project/ai/","section":"project","summary":"Implemented an AI program using Python3 to play Connect4 interactively based on a MinMax strategy. Evaluated board states to make an optimal decision for the next move ","tags":[],"title":"Connect4 board game ","type":"project"},{"authors":null,"categories":null,"content":"import numpy as np\rimport matplotlib.pyplot as plt\r x1=[]\ry1=[5,10,15,20,25,30]\rfor i in range(100):\rx1.append(i)\rprint(x1)\r [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\r plt.hist([10,20,30,40,50],bins=10)\rplt.xlabel(\u0026quot;data\u0026quot;)\rplt.show()\r   ","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"39a81ceb0737d863e6c580c446ed840f","permalink":"https://sushantmhambrey.github.io/post/nlp1/","publishdate":"2019-09-01T00:00:00Z","relpermalink":"/post/nlp1/","section":"post","summary":"import numpy as np\rimport matplotlib.pyplot as plt\r x1=[]\ry1=[5,10,15,20,25,30]\rfor i in range(100):\rx1.append(i)\rprint(x1)\r [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\r plt.","tags":null,"title":"My first title","type":"post"},{"authors":null,"categories":null,"content":"We know that even though linear models can provide good training models rudimentarily, there are lots of situations where the variables don't reveal a linear relationship.\rThus we need to create polynomial models for such datasets.One major issue with polynomial models is that they are suspectible to overfitting.In this article we will look at how a higher degree polynomial model overfits a dataset to create a perfect training environment as opposed to it's errors introduced while testing on the same degree.We will further look at how regularization helps to tackel overfitting and what values of hyperparameters produce the best results.\nImporting the necessary libraries\nimport math\rimport pandas as pd\rimport operator\rimport numpy as np\rfrom sklearn.model_selection import train_test_split\rimport matplotlib.pyplot as plt\rfrom sklearn.metrics import mean_squared_error\rfrom sklearn.linear_model import LinearRegression\rfrom sklearn.linear_model import Ridge\rfrom sklearn.preprocessing import PolynomialFeatures\r#from sklearn.linear_model import Lasso\r#from sklearn.linear_model import ElasticNet\rfrom sklearn.pipeline import make_pipeline\rfrom sklearn.pipeline import Pipeline\rfrom sklearn.preprocessing import StandardScaler\r Creating X,Y data pairs.Here x is sampled from a uniform distribution and N from a gaussian normal distribution\n#we can change the seed value to get different randome numbers for our x,N values\rnp.random.seed(45)\rx=np.random.uniform(low=0,high=1,size=20)\rmu=0\rsigma=1\r#N is based on a gaussian normal distribution\rN=np.random.normal(mu,sigma,size=20)\ry=(np.sin(2*np.pi*x))+N\r #splitting the dataset into 10 for training and 10 for testing\rx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\rx_train\r array([0.97600332, 0.62739168, 0.44053089, 0.99072168, 0.16332445,\r0.07728957, 0.28266721, 0.673068 , 0.47280797, 0.048522 ])\r Displaying weights of polynomial regression within the table\n As we can see the polynomial degree of order 9 has coefficients ranging from x to the power 0 up until x to the power 9 ; a total of 10 coefficients #pip install plotly(plotly not available on some jupyter)\rimport plotly.graph_objects as go\rfrom IPython.display import display\rdef give_weights(x,y,degree):\rpoly_coefficients=np.polyfit(x,y,degree)\rreturn poly_coefficients\r#calling the 'gove_weights' function for degrees of 0,1,6,9\rdegree_0=give_weights(x_train,y_train,0)\rdegree_1=give_weights(x_train,y_train,1)\rdegree_6=give_weights(x_train,y_train,6)\rdegree_9=give_weights(x_train,y_train,9)\r#creating the table for weights\rcoeff=['w0*', 'w1*','w2*', 'w3*', 'w4*', 'w5*', 'w6*', 'w7*', 'w8*','w9*']\r#fig = go.Figure(data=[go.Table(header=dict(values=[' ','M=0', 'M=1','M=3','M=9']),\r#cells=dict(values=[coeff,degree_0,degree_1,degree_6,degree_9])) #df = pd.DataFrame(data=numpy_data,columns=[\u0026quot;w0\u0026quot;])\r#print(df)\r #a reference true fit graph to see how our data fits with different polynomial degrees\rx_rn=np.linspace(0,1,100)\ry_rn=(np.sin(2*np.pi*x_rn))\rplt.scatter(x_train,y_train,s=10,color=\u0026quot;red\u0026quot;)\rplt.plot(x_rn,y_rn)\rplt.show()\r Making the graphs for fit data with the specific degrees ranging from 0 to 9 We can observe that as we increase the degree of our polynomial regression model , the graph tends to cover all the datapoints leading to overfitting from sklearn.pipeline import make_pipeline\r#x = np.sort(x_train[:])\r#y=np.sort(y_train[:])\rtrain_error=[]\rtest_error=[] for i in range(10):\rplt.title('Degree %d' %i)\rplt.text(0.7, .55, 'M=%d' %i)\rX = x_train[:, np.newaxis]\rY = y_train[:, np.newaxis]\rX1 = x_test[:,np.newaxis]\rY1 = y_test[:,np.newaxis]\r#we first make use of the linearregression model to observe how it overfits at higher degrees.\rmodel = make_pipeline(PolynomialFeatures(i), LinearRegression())\rmodel.fit(X,Y)\ry_pred = model.predict(X)\rmse = (mean_squared_error(Y,y_pred))\rrmse=math.sqrt(mse)\rtrain_error.append(rmse)\ry_test_pred=model.predict(X1)\rmse_test= (mean_squared_error(Y1,y_test_pred))\rrmse_test=math.sqrt(mse_test)\rtest_error.append(rmse_test)\r#sorting\rlists=sorted(zip(*[X,y_pred]))\rX,y_pred = list(zip(*lists))\r#plotting the models at various degrees.\rplt.scatter(x_train, y_train,color='black',label='data points')\rplt.plot(X, y_pred, color='g',label='degree_fit')\rplt.plot(x_rn,y_rn,color='r',label='true_fit')\rplt.legend(loc=\u0026quot;lower left\u0026quot;)\r_=plt.xlabel(\u0026quot;X--\u0026gt;\u0026quot;)\r_=plt.ylabel(\u0026quot;t--\u0026gt;\u0026quot;)\rplt.show()\r Plotting training vs test error\rplt.plot(train_error,label='train error')\rplt.plot(test_error,label='test error')\rplt.xticks(np.arange(0, 10, 1.0))\rplt.legend(loc=\u0026quot;upper left\u0026quot;)\r_=plt.xlabel(\u0026quot;M\u0026quot;)\r_=plt.ylabel(\u0026quot;E(RMS)\u0026quot;)\rplt.show()\r Generating 100 data points and fitting ninth order model on it #creating 100 data points\rnp.random.seed(10)\rx2=np.random.uniform(low=0,high=1,size=100)\rmu=0\rsigma=1\rN2=np.random.normal(mu,sigma,size=100)\ry2=(np.sin(2*np.pi*x2))+N2\r x_train100, x_test100, y_train100, y_test100 = train_test_split(x2, y2, test_size=0.01)\r X100 = x_train100[:, np.newaxis]\rY100 = y_train100[:, np.newaxis]\rmodel = make_pipeline(PolynomialFeatures(9), LinearRegression())\r#fitting the 100 data points of the ninth order model.\rmodel.fit(X100,Y100)\ry_pred100 = model.predict(X100)\r#sorting\rlists=sorted(zip(*[X100,y_pred100]))\rX100,y_pred100 = list(zip(*lists))\rplt.scatter(x_train100, y_train100,color='#3299a8',label='data points')\rplt.plot(X100, y_pred100, color='r',label='model fit')\rplt.plot(x_rn,y_rn,color='b',label='true fit')\rplt.legend(loc=\u0026quot;lower left\u0026quot;)\r_=plt.xlabel(\u0026quot;X--\u0026gt;\u0026quot;)\r_=plt.ylabel(\u0026quot;Y--\u0026gt;\u0026quot;)\rplt.show()\r Regularisation and graph creation for different values of lambda\nlbd=[1, 1/10, 1/100, 1/1000, 1/10000, 1/100000]\rplt.ylim(-2,2)\rtrain_error2=[]\rtest_error2=[]\rfor i in range(6):\rmodel1= make_pipeline(StandardScaler(),PolynomialFeatures(degree=9), Ridge(alpha=lbd[i],fit_intercept=True))\rmodel1.fit(X,Y)\ry_pred2 = model1.predict(X)\rmse2 = (mean_squared_error(Y,y_pred2))\rrmse2=math.sqrt(mse2)\rtrain_error2.append(rmse2)\ry_test_pred2=model1.predict(X1)\rmse_test2= (mean_squared_error(Y1,y_test_pred2))\rrmse_test2=math.sqrt(mse_test2)\rtest_error2.append(rmse_test2)\rlists=sorted(zip(*[X,y_pred2]))\rX,y_pred2 = list(zip(*lists))\rplt.scatter(X,Y,color='black')\rplt.plot(X, y_pred2, color='g')\rplt.plot(x_rn,y_rn,color='r')\r_=plt.xlabel(\u0026quot;X--\u0026gt;\u0026quot;)\r_=plt.ylabel(\u0026quot;t--\u0026gt;\u0026quot;)\rplt.show()\r#print('Score: {}'.format(model1.score(X,Y)))\r#print('Test :{}' .format(model1.score(x_test.reshape(-1,1),y_test.reshape(-1,1))))\r Plotting Training vs Test values for various lambda plt.plot(np.log(lbd),train_error2,label='train error')\rplt.plot(np.log(lbd),test_error2,label='test error')\r#plt.xscale(\u0026quot;log\u0026quot;)\r#plt.xticks(np.arange(0, 1.1, 0.1))\rplt.xlim(-10,0)\rplt.legend(loc=\u0026quot;upper right\u0026quot;)\r_=plt.xlabel(\u0026quot;ln(lambda)\u0026quot;)\r_=plt.ylabel(\u0026quot;E(RMS)\u0026quot;)\rplt.show()\r#print(test_error2)\r Based on best test perfomance the Ridge model helps us to regularize our overfitting which we could not do throug LinearRegression. We observe that as the model complexity increases,bias decreases and variance increases and vice versa.Also,we can see that as the lambda value decreases we get a good training score and we obtain the best training score for lambda =1/100000.But thats not the case with the testing score.Before we regularise, we see that the model of degree 9 fits all the data points but leads to overfitting.So a polynoial regression model of degree 6 will be best amongst what we tried without leading to overfit.Also after we regularize we see that the lambda values are almost constant before increasing so the model with lambda 1/1000 seems to be the best  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"af96cd77d249e8f0bfd4e0baa04928df","permalink":"https://sushantmhambrey.github.io/post/overfit/index1/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/overfit/index1/","section":"post","summary":"We know that even though linear models can provide good training models rudimentarily, there are lots of situations where the variables don't reveal a linear relationship.\rThus we need to create polynomial models for such datasets.One major issue with polynomial models is that they are suspectible to overfitting.In this article we will look at how a higher degree polynomial model overfits a dataset to create a perfect training environment as opposed to it's errors introduced while testing on the same degree.","tags":null,"title":"UNDERSTANDING OVERFITTING USING POLYNOMIAL REGRESSION","type":"post"}]
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sushant Mhambrey</title>
    <link>https://sushantmhambrey.github.io/</link>
      <atom:link href="https://sushantmhambrey.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Sushant Mhambrey</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 08 May 2020 05:15:48 -0500</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Sushant Mhambrey</title>
      <link>https://sushantmhambrey.github.io/</link>
    </image>
    
    <item>
      <title>Assignment3</title>
      <link>https://sushantmhambrey.github.io/post/assn3/</link>
      <pubDate>Fri, 08 May 2020 05:15:48 -0500</pubDate>
      <guid>https://sushantmhambrey.github.io/post/assn3/</guid>
      <description> &lt;iframe src=&#34;https://sushantmhambrey.github.io/Mhambrey_03.html&#34; width=&#34;100%&#34;height=&#34;150&#34; style=&#34;border:none;&#34; name=&#34;iframe&#34;&gt;&lt;/iframe&gt;</description>
    </item>
    
    <item>
      <title>NLP</title>
      <link>https://sushantmhambrey.github.io/project/nlp/</link>
      <pubDate>Fri, 06 Mar 2020 04:19:38 -0600</pubDate>
      <guid>https://sushantmhambrey.github.io/project/nlp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Data Pipeline </title>
      <link>https://sushantmhambrey.github.io/project/pipe/</link>
      <pubDate>Wed, 29 Jan 2020 23:00:49 -0600</pubDate>
      <guid>https://sushantmhambrey.github.io/project/pipe/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Virtual Shoe Trial</title>
      <link>https://sushantmhambrey.github.io/project/be/</link>
      <pubDate>Wed, 29 Jan 2020 23:00:39 -0600</pubDate>
      <guid>https://sushantmhambrey.github.io/project/be/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Simulation of Random Variables</title>
      <link>https://sushantmhambrey.github.io/project/damt/</link>
      <pubDate>Wed, 29 Jan 2020 23:00:32 -0600</pubDate>
      <guid>https://sushantmhambrey.github.io/project/damt/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Connect4 board game </title>
      <link>https://sushantmhambrey.github.io/project/ai/</link>
      <pubDate>Wed, 29 Jan 2020 22:20:31 -0600</pubDate>
      <guid>https://sushantmhambrey.github.io/project/ai/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Implementing kNN from scratch on IRIS dataset</title>
      <link>https://sushantmhambrey.github.io/post/assn2/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://sushantmhambrey.github.io/post/assn2/</guid>
      <description>&lt;p style=background-color:#ff7814;font-weight:bold;font-size:16px;text-align:center;&gt; Introduction to kNN&lt;/p&gt;
&lt;p style=&#34;font-family:Georgia;font-size:16px;&#34;&gt;The K-nearest-neighbor (kNN) is one of the most important and simple methods which can be used for both classification and regression problems but is more widely preferred in classification. Although it is simplistic in nature, the KNN algorithm can have better performance levels than many other classifiers’ is usually referred to as a “lazy, non parametric” learning algorithm. A non-parametric technique usually means that it does not assume anything about the data distribution. The structure of the model is defined by the data which is very advantageous when viewed from real world perspective. For these reasons , the rudimentary kNN algorithm can be considered as a good starting point for classification problems containing little or no prior knowledge about distribution data.
One of the most important test cases of kNN can be determining similarity of documents(sometimes referred to as “Concept Search”).Though kNN is easy to use and understand it has its own downfalls.As compared to neural network or SVM , kNN performs really slowly and can sometimes be less accurate.&lt;/p&gt;
&lt;p style=&#34;font-family:Georgia;font-size:15.3px;&#34;&gt;The kNN working is really simple. There is minimal training and heavy testing involved. When we need to make a prediction, the k-most similar neighbors are located and an equivalent prediction is made. It is like forming a “majority vote” between the k most similar instances to a new unobserved instance. Similarity is the distance metric between two data points. There are a number of distance measures available each with better accuracy than other depending on the given use cases and the hyperparameter selections. Usually a good starting point of distance measure in case of tabular data is the “Euclidian distance”&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Exporting the necessary libraries&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np 
import pandas as pd
import seaborn as sns
from collections import Counter
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=&#34;font-family:Georgia;font-size:15px;&#34;&gt;The assignment is about leveraging kNN in Python on a simple classification problem.The dataset at hand is the “Iris Flower Dataset(IFD)”  taken from UC Irvine Machine Learning Repository. The set has 3 Iris species((Iris setosa, Iris virginica and Iris versicolor) each having 50 observations. We have 4 features(attributes): 2 length(sepal_length,petal_length) and 2 width(sepal_width,petal_width)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris = pd.read_csv(&amp;quot;iris.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; Observing the data. Iris dataset consists of four features and one class attribute&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.head()

&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sepal_length&lt;/th&gt;
      &lt;th&gt;sepal_width&lt;/th&gt;
      &lt;th&gt;petal_length&lt;/th&gt;
      &lt;th&gt;petal_width&lt;/th&gt;
      &lt;th&gt;class&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.1&lt;/td&gt;
      &lt;td&gt;3.5&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;Iris-setosa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.9&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;Iris-setosa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4.7&lt;/td&gt;
      &lt;td&gt;3.2&lt;/td&gt;
      &lt;td&gt;1.3&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;Iris-setosa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;4.6&lt;/td&gt;
      &lt;td&gt;3.1&lt;/td&gt;
      &lt;td&gt;1.5&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;Iris-setosa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;3.6&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;Iris-setosa&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.describe()

&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sepal_length&lt;/th&gt;
      &lt;th&gt;sepal_width&lt;/th&gt;
      &lt;th&gt;petal_length&lt;/th&gt;
      &lt;th&gt;petal_width&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;count&lt;/td&gt;
      &lt;td&gt;150.000000&lt;/td&gt;
      &lt;td&gt;150.000000&lt;/td&gt;
      &lt;td&gt;150.000000&lt;/td&gt;
      &lt;td&gt;150.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mean&lt;/td&gt;
      &lt;td&gt;5.843333&lt;/td&gt;
      &lt;td&gt;3.054000&lt;/td&gt;
      &lt;td&gt;3.758667&lt;/td&gt;
      &lt;td&gt;1.198667&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;std&lt;/td&gt;
      &lt;td&gt;0.828066&lt;/td&gt;
      &lt;td&gt;0.433594&lt;/td&gt;
      &lt;td&gt;1.764420&lt;/td&gt;
      &lt;td&gt;0.763161&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;min&lt;/td&gt;
      &lt;td&gt;4.300000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.100000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;25%&lt;/td&gt;
      &lt;td&gt;5.100000&lt;/td&gt;
      &lt;td&gt;2.800000&lt;/td&gt;
      &lt;td&gt;1.600000&lt;/td&gt;
      &lt;td&gt;0.300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;50%&lt;/td&gt;
      &lt;td&gt;5.800000&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;4.350000&lt;/td&gt;
      &lt;td&gt;1.300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;75%&lt;/td&gt;
      &lt;td&gt;6.400000&lt;/td&gt;
      &lt;td&gt;3.300000&lt;/td&gt;
      &lt;td&gt;5.100000&lt;/td&gt;
      &lt;td&gt;1.800000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;max&lt;/td&gt;
      &lt;td&gt;7.900000&lt;/td&gt;
      &lt;td&gt;4.400000&lt;/td&gt;
      &lt;td&gt;6.900000&lt;/td&gt;
      &lt;td&gt;2.500000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; Analysing the data and plotting various iris classes in swarmplot.&lt;/p&gt;
&lt;p style=font-weight:bold;&gt; From the plot it can be observed that iris setosa has the least length range while virginica has the longest range&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.swarmplot(x=&#39;class&#39;,y=&#39;sepal_length&#39;,data=iris)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x16d2639ac88&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_13_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.groupby(&#39;class&#39;).size()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;class
Iris-setosa        50
Iris-versicolor    50
Iris-virginica     50
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.preprocessing import LabelEncoder
feature_columns = [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;,&#39;petal_width&#39;]
X = np.array(iris[feature_columns])
y = np.array(iris[&#39;class&#39;])
np.size(y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;150
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last column of the dataset represents the class values of the corresponding data.We need to convert this into integer for prediction&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;last_column= LabelEncoder()
y = last_column.fit_transform(y)
y
from sklearn.model_selection import train_test_split
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=font-weight:bold;&gt;We can observe that the classes are assigned integer values respectively. Example:: Iris-Setosa becomes 0, Versicolor becomes 1 and Virginica becomes 2 &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size = 0.35)
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; Writing kNN fit from scratch &lt;/p&gt;
&lt;p style=font-weight:bold;&gt; Here is where we implement the actual magic. In a nutshell , a particular row is measure against the rest of the development set and a majority vote is returned.As we have seen earlier kNN is a Lazy algorithm , it doesnt require an implementation of a train function--which is just learning the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from numpy import dot
from numpy.linalg import norm

#----------EUCLIDIAN DISTANCE------------------------
def My_knn_fit(X_dev, y_dev, x_test, k,measure):
    if(measure==&amp;quot;euc&amp;quot;):
        dist_euc = []
        class_output = []

        for i in range(len(X_dev)):
            dist_euc.append([np.sqrt(np.sum(np.square(x_test - X_dev[i, :]))),i])
            
        dist_euc_sort = sorted(dist_euc)

        for i in range(k):
            class_in = dist_euc_sort[i][1]
            class_output.append(y_dev[class_in])
        majority_vote=Counter(class_output).most_common(1)[0][0]
        
        return majority_vote
    
#------------COSINE DISTANCE---------------------------    
    elif(measure==&amp;quot;cosine&amp;quot;):
        dist_cos = []
        class_output = []

        for i in range(len(X_dev)):
            cos_sim = np.sum((dot(x_test,X_dev[i, :]))/(norm(X_dev[i, :])*norm(x_test)))
            cos_dis=(1-cos_sim)
            dist_cos.append([(cos_dis),i])
            
        dist_cos_sort = sorted(dist_cos)
        
        for i in range(k):
            class_in = dist_cos_sort[i][1]
            class_output.append(y_dev[class_in])
        majority_vote=Counter(class_output).most_common(1)[0][0]   
        return majority_vote
    
#------------NORMALIZED-EUCLIDIAN DISTANCE-------------
    elif(measure==&amp;quot;norm_euc&amp;quot;):     
    #0.5*var(X-Y)/var(x)-var(y)
        dist_norm = []
        class_output = []
        for i in range(len(X_dev)):
            dist_norm.append([np.sum((np.var(X_dev[i, :]-x_test)/(np.var(X_dev[i,:]-np.var(x_test))))),i])
    
        dist_norm_sort = sorted(dist_norm)
        for i in range(k):
            class_in = dist_norm_sort[i][1]
            class_output.append(y_dev[class_in])
        majority_vote=Counter(class_output).most_common(1)[0][0]     
        return majority_vote

        
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Implementing kNN predict function &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def My_KNN_predict(X_dev, y_dev, X_test, k):

    pred_euc = []
    pred_cosine=[]
    pred_norm=[]
    for i in range(len(X_test)):
        pred_euc.append(My_knn_fit(X_dev, y_dev, X_test[i, :], k,&amp;quot;euc&amp;quot;))
        pred_cosine.append(My_knn_fit(X_dev, y_dev, X_test[i, :], k,&amp;quot;cosine&amp;quot;))
        pred_norm.append(My_knn_fit(X_dev, y_dev, X_test[i, :], k,&amp;quot;norm_euc&amp;quot;))
    pred_euc=np.array(pred_euc)
    pred_cosine=np.array(pred_cosine)
    pred_norm=np.array(pred_norm)
    #print(&amp;quot;cos:&amp;quot;,pred_cosine)
    #print(&amp;quot;norm:&amp;quot;,pred_norm)
    return (pred_euc,pred_cosine,pred_norm)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; Let&#39;s try out a random input taken from the csv file to see if the predcition is done correctly.The random input taken belong to setosa which corresponds to class 0 &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ran=[5.1,3.5,1.4,0.2]
predin=My_knn_fit(X_dev,y_dev,ran,5,&amp;quot;euc&amp;quot;)
if(predin==0):
    print(&amp;quot;iris-setosa&amp;quot;)
elif(predin==1):
    print(&amp;quot;iris-versicolor&amp;quot;)
else:
    print(&amp;quot;iris-virginica&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;iris-setosa
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def accuracy_scor(y_test, prediction):
    correct = 0
    n = len(y_test)
    for i in range(n):
        if(y_test[i]==prediction[i]):
            correct+=1
    accuracy_sc = (correct*100)/n
    return accuracy_sc
acc_cosine=[]
acc_euc=[]
acc_norm=[]
for k in range(1,8,2):
    pred_euc,pred_cosine,pred_norm = My_KNN_predict(X_dev, y_dev, X_test,k)
    # evaluating accuracy
    accuracy1 = accuracy_scor(y_test, pred_euc)
    acc_euc.append(accuracy1)
    accuracy2=accuracy_scor(y_test, pred_cosine)
    acc_cosine.append(accuracy2)
    accuracy3=accuracy_scor(y_test, pred_norm)
    acc_norm.append(accuracy3)
print(&amp;quot;Euclidian accuracy:&amp;quot;,(acc_euc))
print(&amp;quot;Cosine accuracy:&amp;quot;,(acc_cosine))
print(&amp;quot;Normalised-Euclidian:&amp;quot;,(acc_norm))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Euclidian accuracy: [94.33962264150944, 94.33962264150944, 96.22641509433963, 96.22641509433963]
Cosine accuracy: [92.45283018867924, 96.22641509433963, 98.11320754716981, 98.11320754716981]
Normalised-Euclidian: [96.22641509433963, 98.11320754716981, 98.11320754716981, 98.11320754716981]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k=[1,3,5,7]
sns.set(style=&amp;quot;whitegrid&amp;quot;)
ax=sns.barplot(x=k, y=acc_euc, data=iris)
ax.set(ylim=(85, 100))
ax.set(xlabel=&#39;K-value&#39;, ylabel=&#39;Accuracy(Euclidian)&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[Text(0, 0.5, &#39;Accuracy(Euclidian)&#39;), Text(0.5, 0, &#39;K-value&#39;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_28_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k=[1,3,5,7]
sns.set(style=&amp;quot;whitegrid&amp;quot;)
ax=sns.barplot(x=k, y=acc_cosine, data=iris)
ax.set(ylim=(85,100))
ax.set(xlabel=&#39;K-value&#39;, ylabel=&#39;Accuracy(Cosine)&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[Text(0, 0.5, &#39;Accuracy(Cosine)&#39;), Text(0.5, 0, &#39;K-value&#39;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_29_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k=[1,3,5,7]
sns.set(style=&amp;quot;whitegrid&amp;quot;)
ax=sns.barplot(x=k, y=acc_cosine, data=iris)
ax.set(ylim=(85,100))
ax.set(xlabel=&#39;K-value&#39;, ylabel=&#39;Accuracy(Normalized Euclidian)&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[Text(0, 0.5, &#39;Accuracy(Normalized Euclidian)&#39;), Text(0.5, 0, &#39;K-value&#39;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_30_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=font-weight:bold;&gt;We see the various accuracies we get on euclidian , cosine and normalised euclidian distances with the various k values of 1,3,5,7.Let&#39;s see what happens when we run the fit and predict on hyperparameter K value from 1 to 20&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;acc_cosine1=[]
acc_euc1=[]
acc_norm1=[]
for k in range(1,20,1):
    pred_euc,pred_cosine,pred_norm = My_KNN_predict(X_dev, y_dev, X_test,k)
    # evaluating accuracy
    accuracy1 = accuracy_scor(y_test, pred_euc)
    acc_euc1.append(accuracy1)
    accuracy2=accuracy_scor(y_test, pred_cosine)
    acc_cosine1.append(accuracy2)
    accuracy3=accuracy_scor(y_test, pred_norm)
    acc_norm1.append(accuracy3)
print(&amp;quot;Euclidian accuracy:&amp;quot;,max(acc_euc1),acc_euc1.index(max(acc_euc1))+1)
print(&amp;quot;Cosine accuracy:&amp;quot;,max(acc_cosine1),acc_cosine1.index(max(acc_cosine1))+1)
print(&amp;quot;Normalised-Euclidian:&amp;quot;,max(acc_norm1),acc_norm1.index(max(acc_norm1))+1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Euclidian accuracy: 100.0 7
Cosine accuracy: 100.0 1
Normalised-Euclidian: 100.0 4
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=font-weight:bold;&gt; We can see that considering the given parameters we get optimal values at k=5 for cosine similarity and if we iterate over values of k on 1 to 20 we get perfect scores which maybe due to perfectly labelled data or sometimes overfitting.
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def My_KNN_predict_test(X_dev, y_dev, X_test, k):
    pred_cosine=[]
    for i in range(len(X_test)):
        pred_cosine.append(My_knn_fit(X_dev, y_dev, X_test[i, :], k,&amp;quot;cosine&amp;quot;))
    pred_cosine=np.array(pred_cosine)
    #print(&amp;quot;cos:&amp;quot;,pred_cosine)
    #print(&amp;quot;norm:&amp;quot;,pred_norm)
    return (pred_euc,pred_cosine,pred_norm)
print(&amp;quot;Final accuracy of the optimal hyperparameter wrt to test set:&amp;quot;,acc_cosine[2])
def My_knn_fit_test(X_dev, y_dev, x_test, k,measure):
        if(measure==&amp;quot;cosine&amp;quot;):
            dist_cos = []
            class_output = []

        for i in range(len(X_dev)):
            cos_sim = np.sum((dot(x_test,X_dev[i, :]))/(norm(X_dev[i, :])*norm(x_test)))
            cos_dis=(1-cos_sim)
            dist_cos.append([(cos_dis),i])
            
        dist_cos_sort = sorted(dist_cos)
        
        for i in range(k):
            class_in = dist_cos_sort[i][1]
            class_output.append(y_dev[class_in])
        majority_vote=Counter(class_output).most_common(1)[0][0]   
        return majority_vote
    
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Final accuracy of the optimal hyperparameter wrt to test set: 98.11320754716981
&lt;/code&gt;&lt;/pre&gt;
&lt;p  style=&#34;font-family:Georgia;font-size:16px;&#34;&gt; As we can already see , one of the most advantageous features of kNN apart from being simple and easy to understand is that requires minimal to no training time and serves as a good starting point in learning algorithms.We also saw that though &#34;COSINE SIMILARITY WITH A HYPERPARAMTER OF 5&#34; performed the best with an almost perfect accuracy , it took a lot of time to process as might have compared to other algorithms.Some of the improvements that can be done are using other distance metrics to measure perfomance or using dimensionality reduction techniques depending on the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>UNDERSTANDING OVERFITTING USING POLYNOMIAL REGRESSION</title>
      <link>https://sushantmhambrey.github.io/post/assn1/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://sushantmhambrey.github.io/post/assn1/</guid>
      <description>&lt;p&gt;We know that even though linear models can provide good training models rudimentarily, there are lots of situations where the variables don&amp;rsquo;t reveal a linear relationship. Thus we need to create polynomial models for such datasets.One major issue with polynomial models is that they are suspectible to overfitting.In this article we will look at how a higher degree polynomial model overfits a dataset to create a perfect training environment as opposed to it&amp;rsquo;s errors introduced while testing on the same degree.We will further look at how regularization helps to tackel overfitting and what values of hyperparameters produce the best results.&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Importing the necessary libraries&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math
import pandas as pd
import operator
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
#from sklearn.linear_model import Lasso
#from sklearn.linear_model import ElasticNet
from sklearn.pipeline import make_pipeline
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;
Creating X,Y data pairs.Here x is sampled from a uniform distribution and N from a gaussian normal distribution&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#we can change the seed value to get different randome numbers for our x,N values
np.random.seed(45)
x=np.random.uniform(low=0,high=1,size=20)
mu=0
sigma=1
#N is based on a gaussian normal distribution
N=np.random.normal(mu,sigma,size=20)
y=(np.sin(2*np.pi*x))+N
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#splitting the dataset into 10 for training and 10 for testing
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)
x_train
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0.97600332, 0.62739168, 0.44053089, 0.99072168, 0.16332445,
       0.07728957, 0.28266721, 0.673068  , 0.47280797, 0.048522  ])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#a reference true fit graph to see how our data fits with different polynomial degrees
x_rn=np.linspace(0,1,100)
y_rn=(np.sin(2*np.pi*x_rn))
plt.scatter(x_train,y_train,s=10,color=&amp;quot;red&amp;quot;)
plt.plot(x_rn,y_rn)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_7_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; 
 Making the graphs for fit data with the specific degrees ranging from 0 to 9 &lt;/p&gt;
 &lt;p style=font-weight:bold;&gt; We can observe that as we increase the degree of our polynomial regression model , the graph tends to cover all the datapoints leading to overfitting &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.pipeline import make_pipeline
#x = np.sort(x_train[:])
#y=np.sort(y_train[:])
train_error=[]
test_error=[] 
for i in range(10):
    plt.title(&#39;Degree %d&#39; %i)
    plt.text(0.7, .55, &#39;M=%d&#39; %i)
    X = x_train[:, np.newaxis]
    Y = y_train[:, np.newaxis]
    X1 = x_test[:,np.newaxis]
    Y1 = y_test[:,np.newaxis]
    #we first make use of the linearregression model to observe how it overfits at higher degrees.
    model = make_pipeline(PolynomialFeatures(i), LinearRegression())

    model.fit(X,Y)
    y_pred = model.predict(X)
    mse = (mean_squared_error(Y,y_pred))
    rmse=math.sqrt(mse)
    train_error.append(rmse)
    
    y_test_pred=model.predict(X1)
    mse_test= (mean_squared_error(Y1,y_test_pred))
    rmse_test=math.sqrt(mse_test)
    test_error.append(rmse_test)
    #sorting
    lists=sorted(zip(*[X,y_pred]))
    X,y_pred = list(zip(*lists))
    #plotting the models at various degrees.
    plt.scatter(x_train, y_train,color=&#39;black&#39;,label=&#39;data points&#39;)
    plt.plot(X, y_pred, color=&#39;g&#39;,label=&#39;degree_fit&#39;)
    plt.plot(x_rn,y_rn,color=&#39;r&#39;,label=&#39;true_fit&#39;)
    plt.legend(loc=&amp;quot;lower left&amp;quot;)
    _=plt.xlabel(&amp;quot;X--&amp;gt;&amp;quot;)
    _=plt.ylabel(&amp;quot;t--&amp;gt;&amp;quot;)
    plt.show()
    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_3.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_4.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_5.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_6.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_7.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_8.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_9.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; 
    Plotting training vs test error
    &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.plot(train_error,label=&#39;train error&#39;)
plt.plot(test_error,label=&#39;test error&#39;)
plt.xticks(np.arange(0, 10, 1.0))
plt.legend(loc=&amp;quot;upper left&amp;quot;)
_=plt.xlabel(&amp;quot;M&amp;quot;)
_=plt.ylabel(&amp;quot;E(RMS)&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; 
    Generating 100 data points and fitting ninth order model on it &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#creating 100 data points
np.random.seed(10)
x2=np.random.uniform(low=0,high=1,size=100)
mu=0
sigma=1
N2=np.random.normal(mu,sigma,size=100)
y2=(np.sin(2*np.pi*x2))+N2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_train100, x_test100, y_train100, y_test100 = train_test_split(x2, y2, test_size=0.01)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X100 = x_train100[:, np.newaxis]
Y100 = y_train100[:, np.newaxis]
model = make_pipeline(PolynomialFeatures(9), LinearRegression())

#fitting the 100 data points of the ninth order model.
model.fit(X100,Y100)
y_pred100 = model.predict(X100)

 #sorting
lists=sorted(zip(*[X100,y_pred100]))
X100,y_pred100 = list(zip(*lists))
    
plt.scatter(x_train100, y_train100,color=&#39;#3299a8&#39;,label=&#39;data points&#39;)
plt.plot(X100, y_pred100, color=&#39;r&#39;,label=&#39;model fit&#39;)
plt.plot(x_rn,y_rn,color=&#39;b&#39;,label=&#39;true fit&#39;)
plt.legend(loc=&amp;quot;lower left&amp;quot;)
_=plt.xlabel(&amp;quot;X--&amp;gt;&amp;quot;)
_=plt.ylabel(&amp;quot;Y--&amp;gt;&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; Regularisation and graph creation for different values of lambda&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lbd=[1, 1/10, 1/100, 1/1000, 1/10000, 1/100000]
plt.ylim(-2,2)
train_error2=[]
test_error2=[]
for i in range(6):
    model1= make_pipeline(StandardScaler(),PolynomialFeatures(degree=9), Ridge(alpha=lbd[i],fit_intercept=True))
    
    model1.fit(X,Y)
    y_pred2 = model1.predict(X)
    mse2 = (mean_squared_error(Y,y_pred2))
    rmse2=math.sqrt(mse2)
    train_error2.append(rmse2)
    
    y_test_pred2=model1.predict(X1)
    mse_test2= (mean_squared_error(Y1,y_test_pred2))
    rmse_test2=math.sqrt(mse_test2)
    test_error2.append(rmse_test2)
    
    lists=sorted(zip(*[X,y_pred2]))
    X,y_pred2 = list(zip(*lists))
    
    plt.scatter(X,Y,color=&#39;black&#39;)
    plt.plot(X, y_pred2, color=&#39;g&#39;)
    plt.plot(x_rn,y_rn,color=&#39;r&#39;)
    _=plt.xlabel(&amp;quot;X--&amp;gt;&amp;quot;)
    _=plt.ylabel(&amp;quot;t--&amp;gt;&amp;quot;)
    plt.show()
    #print(&#39;Score: {}&#39;.format(model1.score(X,Y)))
    #print(&#39;Test :{}&#39; .format(model1.score(x_test.reshape(-1,1),y_test.reshape(-1,1))))
    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_17_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_17_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_17_3.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_17_4.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_17_5.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; Plotting Training vs Test values for various lambda &lt;/p&gt; 
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.plot(np.log(lbd),train_error2,label=&#39;train error&#39;)
plt.plot(np.log(lbd),test_error2,label=&#39;test error&#39;)
#plt.xscale(&amp;quot;log&amp;quot;)
#plt.xticks(np.arange(0, 1.1, 0.1))
plt.xlim(-10,0)
plt.legend(loc=&amp;quot;upper right&amp;quot;)
_=plt.xlabel(&amp;quot;ln(lambda)&amp;quot;)
_=plt.ylabel(&amp;quot;E(RMS)&amp;quot;)
plt.show()
#print(test_error2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=&#34;font-family:Georgia;font-size:16px;background-color:#2455d1;color:white;&#34;&gt; Based on best test perfomance the Ridge model helps us to regularize our overfitting which we could not do throug LinearRegression. We observe that as the model complexity increases,bias decreases and variance increases and vice versa.Also,we can see that as the lambda value decreases we get a good training score and we obtain the best training score for lambda =1/100000.But thats not the case with the testing score.Before we regularise, we see that the model of degree 9 fits all the data points but leads to overfitting.So a polynoial regression model of degree 6 will be best amongst what we tried without leading to overfit.Also after we regularize we see that the lambda values are almost constant before increasing so the model with lambda 1/1000 seems to be the best &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>

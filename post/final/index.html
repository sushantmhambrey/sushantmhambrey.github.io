<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Sushant Mhambrey">

  
  
  
    
  
  <meta name="description" content="**Data Mining Project Spring 2020**The purpose of the term project is to predict the ratings given a particular review. Throughout the implementation of the term project many models were tested and based on the various evaluation metrics a model was selected. The dataset is taken from Kaggle BGG review dataset . Due to the vast length of dataset Google Colaboratory was used as the implementation platform#Linking colab to google drive where the dataset is savedfrom google.">

  
  <link rel="alternate" hreflang="en-us" href="https://sushantmhambrey.github.io/post/final/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://sushantmhambrey.github.io/post/final/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Sushant Mhambrey">
  <meta property="og:url" content="https://sushantmhambrey.github.io/post/final/">
  <meta property="og:title" content="Data Mining Term Project | Sushant Mhambrey">
  <meta property="og:description" content="**Data Mining Project Spring 2020**The purpose of the term project is to predict the ratings given a particular review. Throughout the implementation of the term project many models were tested and based on the various evaluation metrics a model was selected. The dataset is taken from Kaggle BGG review dataset . Due to the vast length of dataset Google Colaboratory was used as the implementation platform#Linking colab to google drive where the dataset is savedfrom google."><meta property="og:image" content="img/map[gravatar:%!s(bool=false) shape:circle]">
  <meta property="twitter:image" content="img/map[gravatar:%!s(bool=false) shape:circle]"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-05-12T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-05-12T00:00:00&#43;00:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://sushantmhambrey.github.io/post/final/"
  },
  "headline": "Data Mining Term Project",
  
  "datePublished": "2020-05-12T00:00:00Z",
  "dateModified": "2020-05-12T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Sushant Mhambrey"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Sushant Mhambrey",
    "logo": {
      "@type": "ImageObject",
      "url": "img/https://sushantmhambrey.github.io/"
    }
  },
  "description": "**Data Mining Project Spring 2020**\rThe purpose of the term project is to predict the ratings given a particular review. Throughout the implementation of the term project many models were tested and based on the various evaluation metrics a model was selected. The dataset is taken from Kaggle BGG review dataset . Due to the vast length of dataset Google Colaboratory was used as the implementation platform\r#Linking colab to google drive where the dataset is saved\rfrom google."
}
</script>

  

  


  


  





  <title>Data Mining Term Project | Sushant Mhambrey</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Sushant Mhambrey</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Sushant Mhambrey</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#skills"><span>Skills</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv1.pdf"><span>CV</span></a>
        </li>

        
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/"><span></span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Data Mining Term Project</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    May 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    12 min read
  </span>
  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <h1 >**Data Mining Project Spring 2020**</h1>
<p>The purpose of the term project is to predict the ratings given a particular review. Throughout the implementation of the term project many models were tested and based on the various evaluation metrics a model was selected. The dataset is taken from Kaggle BGG review dataset . Due to the vast length of dataset Google Colaboratory was used as the implementation platform<p>
<pre><code>#Linking colab to google drive where the dataset is saved
from google.colab import drive
import zipfile
#Mount the drive
drive.mount('/content/gdrive')
</code></pre>
<pre><code>Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(&quot;/content/gdrive&quot;, force_remount=True).
</code></pre>
<p>Extracting tar file using zipfile module. We have a huge dataset (around 13 million rows)</p>
<pre><code>zip_ref = zipfile.ZipFile(&quot;/content/gdrive/My Drive/Colab Notebooks/bgg-13m-reviews.zip&quot;, 'r')
zip_ref.extractall(&quot;/tmp&quot;)
zip_ref.close()
</code></pre>
<p> Importing the necessary libraries </p>
<pre><code>import numpy as np
import pandas as pd
import string 
import re 
import nltk
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
</code></pre>
<p>Specifying the path where the csv file is stored</p>
<pre><code>path='/tmp/bgg-13m-reviews.csv'
</code></pre>
<p>Reading the csv file as a data frame using the pandas module.In the read_csv we can specify nrows which indicates how many rows need to be read</p>
<pre><code>df = pd.read_csv(path, delimiter=',')
nRow, nCol = df.shape
#print(f'There are {nRow} rows and {nCol} columns')
</code></pre>
<p>The first part of the project is just analysis of the data file to see how the ratings class is spread. The spread is quite lobsided toward a seven rating which is tackled later in the notebook. Textual data like the comment section of the BGG requires a considerable ammount of preprocessing. Cleaning techniques like removal of punctuation, returing string.lower() and removal of NaN values is used.</p>
<pre><code>df=df.dropna()
#print(df.count())
</code></pre>
<p>Seeing how our data frame looks like.The two main columns we are going to use in the project are comment and rating.</p>
<pre><code>df.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>user</th>
      <th>rating</th>
      <th>comment</th>
      <th>ID</th>
      <th>name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>dougthonus</td>
      <td>10.0</td>
      <td>Currently, this sits on my list as my favorite...</td>
      <td>13</td>
      <td>Catan</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>cypar7</td>
      <td>10.0</td>
      <td>I know it says how many plays, but many, many ...</td>
      <td>13</td>
      <td>Catan</td>
    </tr>
    <tr>
      <th>7</th>
      <td>7</td>
      <td>hreimer</td>
      <td>10.0</td>
      <td>i will never tire of this game.. Awesome</td>
      <td>13</td>
      <td>Catan</td>
    </tr>
    <tr>
      <th>11</th>
      <td>11</td>
      <td>daredevil</td>
      <td>10.0</td>
      <td>This is probably the best game I ever played. ...</td>
      <td>13</td>
      <td>Catan</td>
    </tr>
    <tr>
      <th>16</th>
      <td>16</td>
      <td>hurkle</td>
      <td>10.0</td>
      <td>Fantastic game. Got me hooked on games all ove...</td>
      <td>13</td>
      <td>Catan</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code>df['rating'].hist(bins=10)
plt.xlabel('rating of review')
plt.ylabel('number of reviews')
plt.show()
</code></pre>
<p><img src="./final_14_0.png" alt="png"></p>
<pre><code>plt.figure(figsize=(20, 7))
df['name'].value_counts()[:30].plot(kind='bar',color=&quot;skyblue&quot;)
plt.ylabel('Count of Rating')
plt.title('Top 30 Rated Games')
plt.show()
</code></pre>
<p><img src="./final_15_0.png" alt="png"></p>
<pre><code>from wordcloud import WordCloud
text = df.comment[0]

# Create and generate a word cloud image:
wordcloud = WordCloud(width=400,background_color='black').generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis(&quot;off&quot;)
plt.show()
</code></pre>
<p><img src="./final_16_0.png" alt="png"></p>
<pre><code>from nltk.corpus import stopwords
from sklearn.feature_extraction import text
stop = text.ENGLISH_STOP_WORDS
</code></pre>
<p>Preprocessing the text. We remove white spaces, punctuations, stopwords and make the string lowercase</p>
<pre><code>def remove_noise(text):
    
    text = text.apply(lambda x: &quot; &quot;.join(x.lower() for x in x.split()))
    
    text = text.apply(lambda x: &quot; &quot;.join(x.strip() for x in x.split()))
    
    text = text.str.replace('[^\w\s]', '')

    text = text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
        
    return text
df['comment'] = remove_noise(df['comment'])
</code></pre>
<p>Checking the dataframe after cleaning</p>
<pre><code>df.head()
#print(df.info())
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>user</th>
      <th>rating</th>
      <th>comment</th>
      <th>ID</th>
      <th>name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>dougthonus</td>
      <td>10.0</td>
      <td>currently sits list favorite game</td>
      <td>13</td>
      <td>Catan</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>cypar7</td>
      <td>10.0</td>
      <td>know says plays uncounted liked version best</td>
      <td>13</td>
      <td>Catan</td>
    </tr>
    <tr>
      <th>7</th>
      <td>7</td>
      <td>hreimer</td>
      <td>10.0</td>
      <td>tire game awesome</td>
      <td>13</td>
      <td>Catan</td>
    </tr>
    <tr>
      <th>11</th>
      <td>11</td>
      <td>daredevil</td>
      <td>10.0</td>
      <td>probably best game played requires just thinki...</td>
      <td>13</td>
      <td>Catan</td>
    </tr>
    <tr>
      <th>16</th>
      <td>16</td>
      <td>hurkle</td>
      <td>10.0</td>
      <td>fantastic game got hooked games</td>
      <td>13</td>
      <td>Catan</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code>def get_top_n_words(corpus, n=None):
    vec = CountVectorizer(stop_words = 'english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_words(df['comment'], 30)
for word, freq in common_words:
    #print(word, freq)
  df2 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])
#df2.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(
#kind='bar', yTitle='Count', linecolor='black', title='Top 20 words in review after removing stop words')

</code></pre>
<p>As we can see , after removing the stopwords these are our top 30 words</p>
<pre><code>df2
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ReviewText</th>
      <th>count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>game</td>
      <td>2539119</td>
    </tr>
    <tr>
      <th>1</th>
      <td>play</td>
      <td>743697</td>
    </tr>
    <tr>
      <th>2</th>
      <td>like</td>
      <td>520940</td>
    </tr>
    <tr>
      <th>3</th>
      <td>fun</td>
      <td>517789</td>
    </tr>
    <tr>
      <th>4</th>
      <td>good</td>
      <td>398781</td>
    </tr>
    <tr>
      <th>5</th>
      <td>games</td>
      <td>392353</td>
    </tr>
    <tr>
      <th>6</th>
      <td>players</td>
      <td>369127</td>
    </tr>
    <tr>
      <th>7</th>
      <td>really</td>
      <td>359752</td>
    </tr>
    <tr>
      <th>8</th>
      <td>played</td>
      <td>353415</td>
    </tr>
    <tr>
      <th>9</th>
      <td>great</td>
      <td>329474</td>
    </tr>
    <tr>
      <th>10</th>
      <td>just</td>
      <td>325831</td>
    </tr>
    <tr>
      <th>11</th>
      <td>cards</td>
      <td>318786</td>
    </tr>
    <tr>
      <th>12</th>
      <td>time</td>
      <td>267260</td>
    </tr>
    <tr>
      <th>13</th>
      <td>player</td>
      <td>243939</td>
    </tr>
    <tr>
      <th>14</th>
      <td>rules</td>
      <td>206405</td>
    </tr>
    <tr>
      <th>15</th>
      <td>playing</td>
      <td>205275</td>
    </tr>
    <tr>
      <th>16</th>
      <td>little</td>
      <td>197550</td>
    </tr>
    <tr>
      <th>17</th>
      <td>card</td>
      <td>196163</td>
    </tr>
    <tr>
      <th>18</th>
      <td>dont</td>
      <td>194497</td>
    </tr>
    <tr>
      <th>19</th>
      <td>plays</td>
      <td>190899</td>
    </tr>
    <tr>
      <th>20</th>
      <td>lot</td>
      <td>186532</td>
    </tr>
    <tr>
      <th>21</th>
      <td>better</td>
      <td>185508</td>
    </tr>
    <tr>
      <th>22</th>
      <td>theme</td>
      <td>174505</td>
    </tr>
    <tr>
      <th>23</th>
      <td>bit</td>
      <td>172440</td>
    </tr>
    <tr>
      <th>24</th>
      <td>interesting</td>
      <td>169584</td>
    </tr>
    <tr>
      <th>25</th>
      <td>nice</td>
      <td>165569</td>
    </tr>
    <tr>
      <th>26</th>
      <td>love</td>
      <td>162851</td>
    </tr>
    <tr>
      <th>27</th>
      <td>think</td>
      <td>162404</td>
    </tr>
    <tr>
      <th>28</th>
      <td>best</td>
      <td>145424</td>
    </tr>
    <tr>
      <th>29</th>
      <td>simple</td>
      <td>143521</td>
    </tr>
  </tbody>
</table>
</div>
<pre><code>df2['ReviewText'].value_counts()[:30].plot(kind='bar',color=&quot;skyblue&quot;)
plt.xlabel('rating of review')
plt.ylabel('number of reviews')
plt.show()
</code></pre>
<p><img src="./final_25_0.png" alt="png"></p>
<pre><code>#counting the number of ratings to each number
from collections import Counter
rate = df['rating'].values
rate=rate.astype(int)
</code></pre>
<p>Our data is arranged such that all the ratings are in ascending order thus we need to shuffle the data before splitting into training and test sets</p>
<pre><code>df = df.sample(frac=1).reset_index(drop=True)
</code></pre>
<pre><code>df1=df.sample(n=16000)
print(df1.count())
</code></pre>
<pre><code>Unnamed: 0    16000
user          16000
rating        16000
comment       16000
ID            16000
name          16000
dtype: int64
</code></pre>
<p>Assigning &lsquo;x&rsquo; to the comment section and &lsquo;y&rsquo; to the rating section of our data frame</p>
<pre><code>x=df1['comment'].values
y=np.round(df1['rating'].values)
</code></pre>
<pre><code>from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
</code></pre>
<p>Just a look at how our X_test looks. We need to vectorise the data because ML models require vector represenation of texts</p>
<pre><code>X_test
</code></pre>
<pre><code>array(['solo good players 8 3 players 65 4 players unplayable like wheel whitout classic boring card game unfortunately like control strategy play game player 34 uncontrollable strongly recommend play excellent materials',
       'old town solo',
       'neutral occasion huge toyparty value good production quality game merely ok feels like doesnt realize potentialof game idea suspect replayability novelty value',
       ...,
       'fun chillout warmup game simple rules allow quick learnasyouplay plenty replayability character significantly overpowering decent strategy simple game fun ages issue game elimination player left watch rest battle code fairplay fun stopping ganging single character player sit rest gametime',
       'auction',
       'great light dexterity game play kids definitely takes skill especially lanes like shallow troughs instead flat game equalized somewhat played 4 players making lane longer different ways play game nice supports 24 players'],
      dtype=object)
</code></pre>
<p>For textutal data to be processed it needs to be coverted into a vector matrix representation.But just calculating the count(or frequency) of the terms might result in percieving all the terms as equally important. The TF-IDF vectorizer attenuates the terms whose occurence is common and not of much relevance and gives importance to more relevant terms.Mathematically speaking Tf is ratio of number of times a word occurred in a document to the total number of words in the document. and IDF is the log of total docs divided by docs containing the word.</p>
<p><img src="https://www.joyofdata.de/blog/wp-content/uploads/2014/02/tf-idf.png" alt="alt text"></p>
<p>Vectorising the data</p>
<pre><code>vect = TfidfVectorizer(stop_words = 'english',max_df=0.7,max_features=10000)
x_train_cv = vect.fit_transform(X_train)
x_test_cv = vect.transform(X_test)
</code></pre>
<p>Trying of the different ML modules now</p>
<pre><code>from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import RidgeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.externals import joblib
from sklearn.ensemble import VotingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
</code></pre>
<pre><code>accuracy_all=[]
print(accuracy_all)
</code></pre>
<pre><code>[]
</code></pre>
<p>Our first model is Multinomial Naive Bayes.
One of the most simple Naive Bayes model is the Gaussian Naive Bayes model because it works under the assumption that the data is decribed by the Gaussian distribution but there is also Multinomial Naive Bayes that can be used to get the generative distribution within each label.Here the features are assumed to be generated from a simple multinomia distribution. It is suitale for features that have counts(like ratings in our data) because it describes the probability of observing count among number of various categories.</p>
<p>$$
P(c|d) =P(c) \prod_{k=1}^n P(t_k|c)
$$</p>
<pre><code>#Implementing MultinomialNB
model = MultinomialNB()
model.fit(x_train_cv, y_train.astype('int'))
value = model.predict(x_test_cv)
accuracy_NB=accuracy_score(y_test,value)
print(&quot;accuracy of Naive Bayes&quot;,accuracy_NB)
</code></pre>
<pre><code>accuracy of Naive Bayes 0.2914583333333333
</code></pre>
<pre><code>
disp=plot_confusion_matrix(model, x_test_cv, y_test)
</code></pre>
<p><img src="./final_43_0.png" alt="png"></p>
<pre><code>print(&quot;Classification report for Multinomial Naive Bayes \n&quot;,classification_report(y_test,value))
</code></pre>
<pre><code>Classification report for Multinomial Naive Bayes 
               precision    recall  f1-score   support

         1.0       0.00      0.00      0.00        29
         2.0       0.00      0.00      0.00        75
         3.0       0.00      0.00      0.00       124
         4.0       0.00      0.00      0.00       271
         5.0       0.00      0.00      0.00       359
         6.0       0.30      0.23      0.26       967
         7.0       0.28      0.23      0.25      1059
         8.0       0.29      0.77      0.42      1204
         9.0       0.00      0.00      0.00       445
        10.0       0.00      0.00      0.00       267

    accuracy                           0.29      4800
   macro avg       0.09      0.12      0.09      4800
weighted avg       0.20      0.29      0.22      4800



/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</code></pre>
<p>The next model we use is the Support Vector Machine methodolgy. SVM model is mostly used for discriminative classification as opposed to generative classification.We use the scikit learns SVM module to train our model.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png" alt="alt text"></p>
<pre><code>svm= SVC(kernel='linear',probability=True)
svm.fit(x_train_cv,y_train)
pred=svm.predict(x_test_cv)
accuracy_svm=accuracy_score(y_test,pred)
print(&quot;Accuracy of the SVC model:&quot;,accuracy_svm)
</code></pre>
<pre><code>Accuracy of the SVC model: 0.2975
</code></pre>
<pre><code>print(&quot;Classification report for SVM Model \n&quot;,classification_report(y_test,pred))
</code></pre>
<pre><code>Classification report for SVM Model 
               precision    recall  f1-score   support

         1.0       0.00      0.00      0.00        29
         2.0       0.00      0.00      0.00        75
         3.0       0.00      0.00      0.00       124
         4.0       0.16      0.02      0.03       271
         5.0       0.16      0.03      0.05       359
         6.0       0.29      0.37      0.33       967
         7.0       0.28      0.32      0.30      1059
         8.0       0.32      0.58      0.41      1204
         9.0       0.22      0.01      0.03       445
        10.0       0.26      0.04      0.08       267

    accuracy                           0.30      4800
   macro avg       0.17      0.14      0.12      4800
weighted avg       0.26      0.30      0.25      4800



/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</code></pre>
<p>The next model we tested is the KNN model. It had the lowest accuracy among all other models.The k-nearest neighbors (KNN) algorithm is a supervised machine learning algorithm that can be used to solve both classification and regression problems.
When we need to make a prediction, the k-most similar neighbors are located and an equivalent prediction is made. It is like forming a “majority vote” between the k most similar instances to a new unobserved instance. Similarity is the distance metric between two data points.</p>
<p><img src="https://miro.medium.com/max/800/1*2zYNhLc522h0zftD1zDh2g.png" alt="alt text"></p>
<pre><code>kNN = KNeighborsClassifier(n_neighbors=7)
kNN.fit(x_train_cv,y_train)
pred_k = kNN.predict(x_test_cv)
accuracy_knn = accuracy_score(y_test,pred_k)
print('Accuracy of KNN model',accuracy_knn)
</code></pre>
<pre><code>Accuracy of KNN model 0.21916666666666668
</code></pre>
<pre><code>print(&quot;Classification report for KNN model \n&quot;,classification_report(pred_k,value))
</code></pre>
<pre><code>Classification report for KNN model 
               precision    recall  f1-score   support

         1.0       0.00      0.00      0.00         1
         2.0       0.00      0.00      0.00        12
         3.0       0.00      0.00      0.00        39
         4.0       0.00      0.00      0.00       130
         5.0       1.00      0.01      0.01       178
         6.0       0.28      0.18      0.22      1160
         7.0       0.38      0.20      0.26      1659
         8.0       0.31      0.69      0.42      1410
         9.0       0.00      0.00      0.00       161
        10.0       0.00      0.00      0.00        50

    accuracy                           0.32      4800
   macro avg       0.20      0.11      0.09      4800
weighted avg       0.33      0.32      0.27      4800



/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
</code></pre>
<p>Our next and the last model that we used is the Ridge Classifier.We use the sklearn.linear_model library of scikit learn to import the Ridge Classifier.</p>
<pre><code>clf=RidgeClassifier()
clf.fit(x_train_cv,y_train)
pred1=clf.predict(x_test_cv)
accuracy_ridge=accuracy_score(y_test,pred1)
print(&quot;Accuracy of the ridge classifier&quot;,accuracy_ridge)
</code></pre>
<pre><code>Accuracy of the ridge classifier 0.2833333333333333
</code></pre>
<pre><code>accuracy_all.append(accuracy_NB)
accuracy_all.append(accuracy_svm)
accuracy_all.append(accuracy_ridge)
accuracy_all.append(accuracy_knn)
</code></pre>
<pre><code>import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['NB', 'SVM', 'Ridge','KNN']
students = accuracy_all
ax.bar(langs,students)
plt.ylim(0,0.4,0.01)
plt.show()
</code></pre>
<p><img src="./final_55_0.png" alt="png"></p>
<p>We see that almost all the classifiers have the same accuraccy and even though SVM performs better , it takes a lot of time to process even on small datasets</p>
<h1>Contribution</h1>
Applying smoothing to our classifier and relaxing the problem can get us a better accuracy.
We have a broad range of classes from 1 to 10. Smoothing is applied by allowing a difference of 1 between the predicted and the target classes
<pre><code>def smoothing(predicted,actual):
  counter=0
  for i in range(len(predicted)):
    if(predicted[i]-actual[i]&lt;2):
      counter+=1
  return counter/len(predicted)
</code></pre>
<pre><code>acc_nb_smooth=smoothing(value,y_test)
acc_svm_smooth=smoothing(pred,y_test)
acc_ridge_smooth=smoothing(pred1,y_test)
acc_knn_smooth=smoothing(pred_k,y_test)
print(acc_nb_smooth)
print(acc_svm_smooth)
print(acc_ridge_smooth)
print(acc_knn_smooth)
</code></pre>
<pre><code>0.7316666666666667
0.7908333333333334
0.78125
0.783125
</code></pre>
<pre><code>accuracy_smooth=[]

</code></pre>
<pre><code>accuracy_smooth.append(accuracy_NB)
accuracy_smooth.append(acc_nb_smooth)
accuracy_smooth.append(accuracy_svm)
accuracy_smooth.append(acc_svm_smooth)
accuracy_smooth.append(accuracy_ridge)
accuracy_smooth.append(acc_ridge_smooth)
accuracy_smooth.append(accuracy_knn)
accuracy_smooth.append(acc_knn_smooth)
</code></pre>
<pre><code>print(&quot;MultinomialNB accuracy : {x:.2f} &quot;.format(x=accuracy_NB))
print(&quot;MultinomialNB accuracy after smoothing : {x:.2f} &quot;.format(x=acc_nb_smooth))

print(&quot;SVM accuracy : {x:.2f} &quot;.format(x=accuracy_svm))
print(&quot;SVM accuracy after smoothing : {x:.2f} &quot;.format(x=acc_svm_smooth))

print(&quot;Ridge accuracy : {x:.2f} &quot;.format(x=accuracy_ridge))
print(&quot;Ridge accuracy after smoothing : {x:.2f} &quot;.format(x=acc_ridge_smooth))
</code></pre>
<pre><code>MultinomialNB accuracy : 0.29 
MultinomialNB accuracy after smoothing : 0.73 
SVM accuracy : 0.30 
SVM accuracy after smoothing : 0.79 
Ridge accuracy : 0.28 
Ridge accuracy after smoothing : 0.78 
</code></pre>
<pre><code>fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = ['NB','NBS','svm','svmS','rig','rigS','knn','knnS']
students = accuracy_smooth
ax.bar(langs,students)
plt.ylim(0,1)
plt.show()
</code></pre>
<p><img src="./final_62_0.png" alt="png"></p>
<p>We see that individual algorithms prove to be a little inefficient.Using the concept of Ensemble Learning I combine the fastest model with the most efficient model to see if there is an increase in the accuracy.Rather than making use of one model , a weighted model is used.</p>
<p><img src="https://image.slidesharecdn.com/ensemblelearning-120730220523-phpapp02/95/ensemble-learning-the-wisdom-of-crowds-of-machines-15-728.jpg?cb=1343685993" alt="alt text"></p>
<pre><code>EB = VotingClassifier(estimators=[('Linear',svm), ('Nb',model)])
</code></pre>
<p>Fitting our data to the model.A small problem can be that Linear SVM has a predict_prob function that is set to False by default , which we need to set to True</p>
<pre><code>Ensemble.fit(x_train_cv,y_train)
</code></pre>
<pre><code>VotingClassifier(estimators=[('Linear',
                              SVC(C=1.0, break_ties=False, cache_size=200,
                                  class_weight=None, coef0=0.0,
                                  decision_function_shape='ovr', degree=3,
                                  gamma='scale', kernel='linear', max_iter=-1,
                                  probability=True, random_state=None,
                                  shrinking=True, tol=0.001, verbose=False)),
                             ('Nb',
                              MultinomialNB(alpha=1.0, class_prior=None,
                                            fit_prior=True))],
                 flatten_transform=True, n_jobs=None, voting='hard',
                 weights=None)
</code></pre>
<pre><code>e_pred=Ensemble.predict(x_test_cv)
</code></pre>
<pre><code>acc3=accuracy_score(e_pred,y_test)
print(acc3)
acc3_smooth=smoothing(e_pred,y_test)
print(acc3_smooth)
</code></pre>
<pre><code>0.29854166666666665
0.7979166666666667
</code></pre>
<pre><code>print(&quot;Final accuracy on test data after applying smoothin and ensemble methods \n&quot;,acc3_smooth)
</code></pre>
<pre><code>Final accuracy on test data after applying smoothin and ensemble methods 
 0.7979166666666667
</code></pre>
<pre><code>
</code></pre>
<h1>Challenges and Contributions</h1>
One of the more general challenges that I faced was tackling such a huge dataset.Using the method of sampling worked but had some variations in accuracy.Thus a more scaled data in terms of ratings did the job.
Some specific challenge that I faced was to decide whether to use Word2Vec word embedding model. On further testing , it was seen that word embeddings actually created a lower accuracy.Doc2Vec word embeddings could have worked on the larger dataset if the accurate computation was present.On smaller dataset though, it made no difference. While creating ensembles,parameters of the individual models had to be tuned and dimensions had to be adjusted to properly fit our input to the model.Applying smoothing was one more contribution which helped to increase the accuracy.Had to relax certain terms of the model for that.
Even though the database is large , it is more lobsided toward the ratings of 6 7 8 and scaling down the data does the trick.Overall, working on the dataset helped to understand key concepts from simple terms like creating charts of top 20 words or scaling down the data to more complex methodologies like smoothing and ensembles.
<h1>References</h1>
<p><a href="https://towardsdatascience.com/review-rating-prediction-a-combined-approach-538c617c495c">https://towardsdatascience.com/review-rating-prediction-a-combined-approach-538c617c495c</a></p>
<p><a href="https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f">https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f</a></p>
<p><a href="https://www.kaggle.com/josh24990/nlp-ml-which-words-predict-a-recommendation">https://www.kaggle.com/josh24990/nlp-ml-which-words-predict-a-recommendation</a></p>
<p><a href="https://www.developintelligence.com/blog/2017/03/predicting-yelp-star-ratings-review-text-python/">https://www.developintelligence.com/blog/2017/03/predicting-yelp-star-ratings-review-text-python/</a></p>
<pre><code>
</code></pre>

    </div>

    







<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://sushantmhambrey.github.io/post/final/&amp;text=Data%20Mining%20Term%20Project" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://sushantmhambrey.github.io/post/final/&amp;t=Data%20Mining%20Term%20Project" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Data%20Mining%20Term%20Project&amp;body=https://sushantmhambrey.github.io/post/final/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://sushantmhambrey.github.io/post/final/&amp;title=Data%20Mining%20Term%20Project" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Data%20Mining%20Term%20Project%20https://sushantmhambrey.github.io/post/final/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://sushantmhambrey.github.io/post/final/&amp;title=Data%20Mining%20Term%20Project" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu1739f3142890920234273aae3e22735e_279817_270x270_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://sushantmhambrey.github.io/">Sushant Mhambrey</a></h5>
      <h6 class="card-subtitle">Student, Masters of Science in Computer Science</h6>
      <p class="card-text">My research interests include distributed robotics, mobile computing and programmable matter.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:sushantshirish.mhambrey@mavs.uta.edu" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/sushant-mhambrey-2b23731a0/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/sushantmhambrey" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.0630fec5958cb075a5a38f042b3ddde6.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>

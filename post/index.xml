<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Sushant Mhambrey</title>
    <link>https://sushantmhambrey.github.io/post/</link>
      <atom:link href="https://sushantmhambrey.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 12 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Posts</title>
      <link>https://sushantmhambrey.github.io/post/</link>
    </image>
    
    <item>
      <title>Data Mining Term Project</title>
      <link>https://sushantmhambrey.github.io/post/final/</link>
      <pubDate>Tue, 12 May 2020 00:00:00 +0000</pubDate>
      <guid>https://sushantmhambrey.github.io/post/final/</guid>
      <description>&lt;h1 style=background-color:#ff7814;font-weight:bold;font-size:16px;text-align:center;&gt;**Data Mining Project Spring 2020**&lt;/h1&gt;
&lt;p&gt;The purpose of the term project is to predict the ratings given a particular review. Throughout the implementation of the term project many models were tested and based on the various evaluation metrics a model was selected. The dataset is taken from Kaggle BGG review dataset . Due to the vast length of dataset Google Colaboratory was used as the implementation platform&lt;p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Linking colab to google drive where the dataset is saved
from google.colab import drive
import zipfile
#Mount the drive
drive.mount(&#39;/content/gdrive&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(&amp;quot;/content/gdrive&amp;quot;, force_remount=True).
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Extracting tar file using zipfile module. We have a huge dataset (around 13 million rows)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;zip_ref = zipfile.ZipFile(&amp;quot;/content/gdrive/My Drive/Colab Notebooks/bgg-13m-reviews.zip&amp;quot;, &#39;r&#39;)
zip_ref.extractall(&amp;quot;/tmp&amp;quot;)
zip_ref.close()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; Importing the necessary libraries &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
import string 
import re 
import nltk
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specifying the path where the csv file is stored&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;path=&#39;/tmp/bgg-13m-reviews.csv&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reading the csv file as a data frame using the pandas module.In the read_csv we can specify nrows which indicates how many rows need to be read&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = pd.read_csv(path, delimiter=&#39;,&#39;)
nRow, nCol = df.shape
#print(f&#39;There are {nRow} rows and {nCol} columns&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first part of the project is just analysis of the data file to see how the ratings class is spread. The spread is quite lobsided toward a seven rating which is tackled later in the notebook. Textual data like the comment section of the BGG requires a considerable ammount of preprocessing. Cleaning techniques like removal of punctuation, returing string.lower() and removal of NaN values is used.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df=df.dropna()
#print(df.count())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Seeing how our data frame looks like.The two main columns we are going to use in the project are comment and rating.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;user&lt;/th&gt;
      &lt;th&gt;rating&lt;/th&gt;
      &lt;th&gt;comment&lt;/th&gt;
      &lt;th&gt;ID&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;dougthonus&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;Currently, this sits on my list as my favorite...&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Catan&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;cypar7&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;I know it says how many plays, but many, many ...&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Catan&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;hreimer&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;i will never tire of this game.. Awesome&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Catan&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;daredevil&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;This is probably the best game I ever played. ...&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Catan&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;hurkle&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;Fantastic game. Got me hooked on games all ove...&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Catan&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df[&#39;rating&#39;].hist(bins=10)
plt.xlabel(&#39;rating of review&#39;)
plt.ylabel(&#39;number of reviews&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./final_14_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure(figsize=(20, 7))
df[&#39;name&#39;].value_counts()[:30].plot(kind=&#39;bar&#39;,color=&amp;quot;skyblue&amp;quot;)
plt.ylabel(&#39;Count of Rating&#39;)
plt.title(&#39;Top 30 Rated Games&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./final_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from wordcloud import WordCloud
text = df.comment[0]

# Create and generate a word cloud image:
wordcloud = WordCloud(width=400,background_color=&#39;black&#39;).generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation=&#39;bilinear&#39;)
plt.axis(&amp;quot;off&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./final_16_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from nltk.corpus import stopwords
from sklearn.feature_extraction import text
stop = text.ENGLISH_STOP_WORDS
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Preprocessing the text. We remove white spaces, punctuations, stopwords and make the string lowercase&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def remove_noise(text):
    
    text = text.apply(lambda x: &amp;quot; &amp;quot;.join(x.lower() for x in x.split()))
    
    text = text.apply(lambda x: &amp;quot; &amp;quot;.join(x.strip() for x in x.split()))
    
    text = text.str.replace(&#39;[^\w\s]&#39;, &#39;&#39;)

    text = text.apply(lambda x: &#39; &#39;.join([word for word in x.split() if word not in (stop)]))
        
    return text
df[&#39;comment&#39;] = remove_noise(df[&#39;comment&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Checking the dataframe after cleaning&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df.head()
#print(df.info())
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Unnamed: 0&lt;/th&gt;
      &lt;th&gt;user&lt;/th&gt;
      &lt;th&gt;rating&lt;/th&gt;
      &lt;th&gt;comment&lt;/th&gt;
      &lt;th&gt;ID&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;dougthonus&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;currently sits list favorite game&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Catan&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;cypar7&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;know says plays uncounted liked version best&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Catan&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;hreimer&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;tire game awesome&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Catan&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;daredevil&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;probably best game played requires just thinki...&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Catan&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;hurkle&lt;/td&gt;
      &lt;td&gt;10.0&lt;/td&gt;
      &lt;td&gt;fantastic game got hooked games&lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Catan&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def get_top_n_words(corpus, n=None):
    vec = CountVectorizer(stop_words = &#39;english&#39;).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_words(df[&#39;comment&#39;], 30)
for word, freq in common_words:
    #print(word, freq)
  df2 = pd.DataFrame(common_words, columns = [&#39;ReviewText&#39; , &#39;count&#39;])
#df2.groupby(&#39;ReviewText&#39;).sum()[&#39;count&#39;].sort_values(ascending=False).iplot(
#kind=&#39;bar&#39;, yTitle=&#39;Count&#39;, linecolor=&#39;black&#39;, title=&#39;Top 20 words in review after removing stop words&#39;)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see , after removing the stopwords these are our top 30 words&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df2
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ReviewText&lt;/th&gt;
      &lt;th&gt;count&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;game&lt;/td&gt;
      &lt;td&gt;2539119&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;play&lt;/td&gt;
      &lt;td&gt;743697&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;like&lt;/td&gt;
      &lt;td&gt;520940&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;fun&lt;/td&gt;
      &lt;td&gt;517789&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;good&lt;/td&gt;
      &lt;td&gt;398781&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;games&lt;/td&gt;
      &lt;td&gt;392353&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;players&lt;/td&gt;
      &lt;td&gt;369127&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;really&lt;/td&gt;
      &lt;td&gt;359752&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;played&lt;/td&gt;
      &lt;td&gt;353415&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;great&lt;/td&gt;
      &lt;td&gt;329474&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;just&lt;/td&gt;
      &lt;td&gt;325831&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;cards&lt;/td&gt;
      &lt;td&gt;318786&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;time&lt;/td&gt;
      &lt;td&gt;267260&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;player&lt;/td&gt;
      &lt;td&gt;243939&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;rules&lt;/td&gt;
      &lt;td&gt;206405&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;playing&lt;/td&gt;
      &lt;td&gt;205275&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;little&lt;/td&gt;
      &lt;td&gt;197550&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;17&lt;/th&gt;
      &lt;td&gt;card&lt;/td&gt;
      &lt;td&gt;196163&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;dont&lt;/td&gt;
      &lt;td&gt;194497&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;19&lt;/th&gt;
      &lt;td&gt;plays&lt;/td&gt;
      &lt;td&gt;190899&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;20&lt;/th&gt;
      &lt;td&gt;lot&lt;/td&gt;
      &lt;td&gt;186532&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;21&lt;/th&gt;
      &lt;td&gt;better&lt;/td&gt;
      &lt;td&gt;185508&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;22&lt;/th&gt;
      &lt;td&gt;theme&lt;/td&gt;
      &lt;td&gt;174505&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;23&lt;/th&gt;
      &lt;td&gt;bit&lt;/td&gt;
      &lt;td&gt;172440&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;24&lt;/th&gt;
      &lt;td&gt;interesting&lt;/td&gt;
      &lt;td&gt;169584&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;25&lt;/th&gt;
      &lt;td&gt;nice&lt;/td&gt;
      &lt;td&gt;165569&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;26&lt;/th&gt;
      &lt;td&gt;love&lt;/td&gt;
      &lt;td&gt;162851&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;27&lt;/th&gt;
      &lt;td&gt;think&lt;/td&gt;
      &lt;td&gt;162404&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;28&lt;/th&gt;
      &lt;td&gt;best&lt;/td&gt;
      &lt;td&gt;145424&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;29&lt;/th&gt;
      &lt;td&gt;simple&lt;/td&gt;
      &lt;td&gt;143521&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df2[&#39;ReviewText&#39;].value_counts()[:30].plot(kind=&#39;bar&#39;,color=&amp;quot;skyblue&amp;quot;)
plt.xlabel(&#39;rating of review&#39;)
plt.ylabel(&#39;number of reviews&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./final_25_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#counting the number of ratings to each number
from collections import Counter
rate = df[&#39;rating&#39;].values
rate=rate.astype(int)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our data is arranged such that all the ratings are in ascending order thus we need to shuffle the data before splitting into training and test sets&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df = df.sample(frac=1).reset_index(drop=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;df1=df.sample(n=16000)
print(df1.count())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Unnamed: 0    16000
user          16000
rating        16000
comment       16000
ID            16000
name          16000
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Assigning &amp;lsquo;x&amp;rsquo; to the comment section and &amp;lsquo;y&amp;rsquo; to the rating section of our data frame&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x=df1[&#39;comment&#39;].values
y=np.round(df1[&#39;rating&#39;].values)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just a look at how our X_test looks. We need to vectorise the data because ML models require vector represenation of texts&lt;/p&gt;
&lt;p&gt;For textutal data to be processed it needs to be coverted into a vector matrix representation.But just calculating the count(or frequency) of the terms might result in percieving all the terms as equally important. The TF-IDF vectorizer attenuates the terms whose occurence is common and not of much relevance and gives importance to more relevant terms.Mathematically speaking Tf is ratio of number of times a word occurred in a document to the total number of words in the document. and IDF is the log of total docs divided by docs containing the word.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.joyofdata.de/blog/wp-content/uploads/2014/02/tf-idf.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt;
&lt;p&gt;Vectorising the data&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vect = TfidfVectorizer(stop_words = &#39;english&#39;,max_df=0.7,max_features=10000)
x_train_cv = vect.fit_transform(X_train)
x_test_cv = vect.transform(X_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Trying of the different ML modules now&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import RidgeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.externals import joblib
from sklearn.ensemble import VotingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy_all=[]
print(accuracy_all)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our first model is Multinomial Naive Bayes.
One of the most simple Naive Bayes model is the Gaussian Naive Bayes model because it works under the assumption that the data is decribed by the Gaussian distribution but there is also Multinomial Naive Bayes that can be used to get the generative distribution within each label.Here the features are assumed to be generated from a simple multinomia distribution. It is suitale for features that have counts(like ratings in our data) because it describes the probability of observing count among number of various categories.&lt;/p&gt;
&lt;p&gt;$$
P(c|d) =P(c) \prod_{k=1}^n P(t_k|c)
$$&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Implementing MultinomialNB
model = MultinomialNB()
model.fit(x_train_cv, y_train.astype(&#39;int&#39;))
value = model.predict(x_test_cv)
accuracy_NB=accuracy_score(y_test,value)
print(&amp;quot;accuracy of Naive Bayes&amp;quot;,accuracy_NB)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;accuracy of Naive Bayes 0.2914583333333333
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
disp=plot_confusion_matrix(model, x_test_cv, y_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./final_43_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Classification report for Multinomial Naive Bayes \n&amp;quot;,classification_report(y_test,value))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Classification report for Multinomial Naive Bayes 
               precision    recall  f1-score   support

         1.0       0.00      0.00      0.00        29
         2.0       0.00      0.00      0.00        75
         3.0       0.00      0.00      0.00       124
         4.0       0.00      0.00      0.00       271
         5.0       0.00      0.00      0.00       359
         6.0       0.30      0.23      0.26       967
         7.0       0.28      0.23      0.25      1059
         8.0       0.29      0.77      0.42      1204
         9.0       0.00      0.00      0.00       445
        10.0       0.00      0.00      0.00       267

    accuracy                           0.29      4800
   macro avg       0.09      0.12      0.09      4800
weighted avg       0.20      0.29      0.22      4800



/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next model we use is the Support Vector Machine methodolgy. SVM model is mostly used for discriminative classification as opposed to generative classification.We use the scikit learns SVM module to train our model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;svm= SVC(kernel=&#39;linear&#39;,probability=True)
svm.fit(x_train_cv,y_train)
pred=svm.predict(x_test_cv)
accuracy_svm=accuracy_score(y_test,pred)
print(&amp;quot;Accuracy of the SVC model:&amp;quot;,accuracy_svm)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy of the SVC model: 0.2975
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Classification report for SVM Model \n&amp;quot;,classification_report(y_test,pred))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Classification report for SVM Model 
               precision    recall  f1-score   support

         1.0       0.00      0.00      0.00        29
         2.0       0.00      0.00      0.00        75
         3.0       0.00      0.00      0.00       124
         4.0       0.16      0.02      0.03       271
         5.0       0.16      0.03      0.05       359
         6.0       0.29      0.37      0.33       967
         7.0       0.28      0.32      0.30      1059
         8.0       0.32      0.58      0.41      1204
         9.0       0.22      0.01      0.03       445
        10.0       0.26      0.04      0.08       267

    accuracy                           0.30      4800
   macro avg       0.17      0.14      0.12      4800
weighted avg       0.26      0.30      0.25      4800



/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next model we tested is the KNN model. It had the lowest accuracy among all other models.The k-nearest neighbors (KNN) algorithm is a supervised machine learning algorithm that can be used to solve both classification and regression problems.
When we need to make a prediction, the k-most similar neighbors are located and an equivalent prediction is made. It is like forming a “majority vote” between the k most similar instances to a new unobserved instance. Similarity is the distance metric between two data points.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/800/1*2zYNhLc522h0zftD1zDh2g.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;kNN = KNeighborsClassifier(n_neighbors=7)
kNN.fit(x_train_cv,y_train)
pred_k = kNN.predict(x_test_cv)
accuracy_knn = accuracy_score(y_test,pred_k)
print(&#39;Accuracy of KNN model&#39;,accuracy_knn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy of KNN model 0.21916666666666668
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Classification report for KNN model \n&amp;quot;,classification_report(pred_k,value))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Classification report for KNN model 
               precision    recall  f1-score   support

         1.0       0.00      0.00      0.00         1
         2.0       0.00      0.00      0.00        12
         3.0       0.00      0.00      0.00        39
         4.0       0.00      0.00      0.00       130
         5.0       1.00      0.01      0.01       178
         6.0       0.28      0.18      0.22      1160
         7.0       0.38      0.20      0.26      1659
         8.0       0.31      0.69      0.42      1410
         9.0       0.00      0.00      0.00       161
        10.0       0.00      0.00      0.00        50

    accuracy                           0.32      4800
   macro avg       0.20      0.11      0.09      4800
weighted avg       0.33      0.32      0.27      4800



/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our next and the last model that we used is the Ridge Classifier.We use the sklearn.linear_model library of scikit learn to import the Ridge Classifier.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;clf=RidgeClassifier()
clf.fit(x_train_cv,y_train)
pred1=clf.predict(x_test_cv)
accuracy_ridge=accuracy_score(y_test,pred1)
print(&amp;quot;Accuracy of the ridge classifier&amp;quot;,accuracy_ridge)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy of the ridge classifier 0.2833333333333333
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy_all.append(accuracy_NB)
accuracy_all.append(accuracy_svm)
accuracy_all.append(accuracy_ridge)
accuracy_all.append(accuracy_knn)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = [&#39;NB&#39;, &#39;SVM&#39;, &#39;Ridge&#39;,&#39;KNN&#39;]
students = accuracy_all
ax.bar(langs,students)
plt.ylim(0,0.4,0.01)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./final_55_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see that almost all the classifiers have the same accuraccy and even though SVM performs better , it takes a lot of time to process even on small datasets&lt;/p&gt;
&lt;h1&gt;Contribution&lt;/h1&gt;
Applying smoothing to our classifier and relaxing the problem can get us a better accuracy.
We have a broad range of classes from 1 to 10. Smoothing is applied by allowing a difference of 1 between the predicted and the target classes
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def smoothing(predicted,actual):
  counter=0
  for i in range(len(predicted)):
    if(predicted[i]-actual[i]&amp;lt;2):
      counter+=1
  return counter/len(predicted)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;acc_nb_smooth=smoothing(value,y_test)
acc_svm_smooth=smoothing(pred,y_test)
acc_ridge_smooth=smoothing(pred1,y_test)
acc_knn_smooth=smoothing(pred_k,y_test)
print(acc_nb_smooth)
print(acc_svm_smooth)
print(acc_ridge_smooth)
print(acc_knn_smooth)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.7316666666666667
0.7908333333333334
0.78125
0.783125
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy_smooth=[]

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy_smooth.append(accuracy_NB)
accuracy_smooth.append(acc_nb_smooth)
accuracy_smooth.append(accuracy_svm)
accuracy_smooth.append(acc_svm_smooth)
accuracy_smooth.append(accuracy_ridge)
accuracy_smooth.append(acc_ridge_smooth)
accuracy_smooth.append(accuracy_knn)
accuracy_smooth.append(acc_knn_smooth)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;MultinomialNB accuracy : {x:.2f} &amp;quot;.format(x=accuracy_NB))
print(&amp;quot;MultinomialNB accuracy after smoothing : {x:.2f} &amp;quot;.format(x=acc_nb_smooth))

print(&amp;quot;SVM accuracy : {x:.2f} &amp;quot;.format(x=accuracy_svm))
print(&amp;quot;SVM accuracy after smoothing : {x:.2f} &amp;quot;.format(x=acc_svm_smooth))

print(&amp;quot;Ridge accuracy : {x:.2f} &amp;quot;.format(x=accuracy_ridge))
print(&amp;quot;Ridge accuracy after smoothing : {x:.2f} &amp;quot;.format(x=acc_ridge_smooth))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MultinomialNB accuracy : 0.29 
MultinomialNB accuracy after smoothing : 0.73 
SVM accuracy : 0.30 
SVM accuracy after smoothing : 0.79 
Ridge accuracy : 0.28 
Ridge accuracy after smoothing : 0.78 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
langs = [&#39;NB&#39;,&#39;NBS&#39;,&#39;svm&#39;,&#39;svmS&#39;,&#39;rig&#39;,&#39;rigS&#39;,&#39;knn&#39;,&#39;knnS&#39;]
students = accuracy_smooth
ax.bar(langs,students)
plt.ylim(0,1)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./final_62_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see that individual algorithms prove to be a little inefficient.Using the concept of Ensemble Learning I combine the fastest model with the most efficient model to see if there is an increase in the accuracy.Rather than making use of one model , a weighted model is used.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://image.slidesharecdn.com/ensemblelearning-120730220523-phpapp02/95/ensemble-learning-the-wisdom-of-crowds-of-machines-15-728.jpg?cb=1343685993&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;EB = VotingClassifier(estimators=[(&#39;Linear&#39;,svm), (&#39;Nb&#39;,model)])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Fitting our data to the model.A small problem can be that Linear SVM has a predict_prob function that is set to False by default , which we need to set to True&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Ensemble.fit(x_train_cv,y_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;VotingClassifier(estimators=[(&#39;Linear&#39;,
                              SVC(C=1.0, break_ties=False, cache_size=200,
                                  class_weight=None, coef0=0.0,
                                  decision_function_shape=&#39;ovr&#39;, degree=3,
                                  gamma=&#39;scale&#39;, kernel=&#39;linear&#39;, max_iter=-1,
                                  probability=True, random_state=None,
                                  shrinking=True, tol=0.001, verbose=False)),
                             (&#39;Nb&#39;,
                              MultinomialNB(alpha=1.0, class_prior=None,
                                            fit_prior=True))],
                 flatten_transform=True, n_jobs=None, voting=&#39;hard&#39;,
                 weights=None)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;e_pred=Ensemble.predict(x_test_cv)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;acc3=accuracy_score(e_pred,y_test)
print(acc3)
acc3_smooth=smoothing(e_pred,y_test)
print(acc3_smooth)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.29854166666666665
0.7979166666666667
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;Final accuracy on test data after applying smoothin and ensemble methods \n&amp;quot;,acc3_smooth)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Final accuracy on test data after applying smoothin and ensemble methods 
 0.7979166666666667
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Challenges and Contributions&lt;/h1&gt;
One of the more general challenges that I faced was tackling such a huge dataset.Using the method of sampling worked but had some variations in accuracy.Thus a more scaled data in terms of ratings did the job.
Some specific challenge that I faced was to decide whether to use Word2Vec word embedding model. On further testing , it was seen that word embeddings actually created a lower accuracy.Doc2Vec word embeddings could have worked on the larger dataset if the accurate computation was present.On smaller dataset though, it made no difference. While creating ensembles,parameters of the individual models had to be tuned and dimensions had to be adjusted to properly fit our input to the model.Applying smoothing was one more contribution which helped to increase the accuracy.Had to relax certain terms of the model for that.
Even though the database is large , it is more lobsided toward the ratings of 6 7 8 and scaling down the data does the trick.Overall, working on the dataset helped to understand key concepts from simple terms like creating charts of top 20 words or scaling down the data to more complex methodologies like smoothing and ensembles.
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/review-rating-prediction-a-combined-approach-538c617c495c&#34;&gt;https://towardsdatascience.com/review-rating-prediction-a-combined-approach-538c617c495c&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f&#34;&gt;https://towardsdatascience.com/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/josh24990/nlp-ml-which-words-predict-a-recommendation&#34;&gt;https://www.kaggle.com/josh24990/nlp-ml-which-words-predict-a-recommendation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.developintelligence.com/blog/2017/03/predicting-yelp-star-ratings-review-text-python/&#34;&gt;https://www.developintelligence.com/blog/2017/03/predicting-yelp-star-ratings-review-text-python/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bayes from Scratch</title>
      <link>https://sushantmhambrey.github.io/post/assn3/</link>
      <pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://sushantmhambrey.github.io/post/assn3/</guid>
      <description>&lt;p style=background-color:#ff7814;font-weight:bold;font-size:16px;text-align:center;&gt; IMPLEMENTING NAIVE BAYES FROM SCRATCH USING MOVIE REVIEW DATASET&lt;/p&gt;
&lt;p style=&#34;font-family:Georgia;font-size:18px;background-color:#2455d1;color:white;&#34;&gt;Naive Bayes is one of the most common ML algorithms that is often used for the purpose of text classification. If you have just stepped into ML, it is one of the easiest classification algorithms to start with. Naive Bayes is a probabilistic classification algorithm as it uses probability to make predictions for the purpose of classification.&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Importing the necessary libraries&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
import os
import string
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sys import path 
from os import listdir
from os.path import join
import re
def Load_Text_Files(dir_path):
    #dir_path=path[0]
    dir_path=review_dir_path
    for file in listdir(dir_path):
        if &#39;.txt&#39; in file:
            paths = [join(dir_path,data)]
    docs = []
    for path in paths:
        with open(path, &#39;r&#39;) as file: docs.append(file.read())
    return docs

def Clean(review):
    p=re.sub(re.compile(&#39;&amp;lt;.*?&amp;gt;&#39;),&#39;&#39;,review)
    rev=re.sub(&#39;[^0-9a-zA-Z]+&#39;, &#39; &#39;, review.lower())
    return rev
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import glob
data_train=[]
for file in glob.glob(os.path.join(&#39;train/neg/&#39;,&#39;*.txt&#39;)):
    f = open(file,&#39;r&#39;,encoding=&amp;quot;utf8&amp;quot;)
    review = f.read()
    data_train.append([review,0])
    
for file in glob.glob(os.path.join(&#39;train/pos/&#39;,&#39;*.txt&#39;)):
    f = open(file,&#39;r&#39;,encoding=&amp;quot;utf8&amp;quot;)
    review = f.read()
    data_train.append([review,1])

data_train = pd.DataFrame(data_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_train.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;th&gt;1&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Story of a man who has unnatural feelings for ...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;OK its not the best film I&#39;ve ever seen but at...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Amateur, no budget films can be surprisingly g...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;My girlfriend once brought around The Zombie C...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Without wishing to be a killjoy, Brad Sykes is...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_test=[]
for file in glob.glob(os.path.join(&#39;test/neg/&#39;,&#39;*.txt&#39;)):
    f = open(file,&#39;r&#39;,encoding=&amp;quot;utf8&amp;quot;)
    review = f.read()
    data_test.append([review,0])
    
for file in glob.glob(os.path.join(&#39;test/pos/&#39;,&#39;*.txt&#39;)):
    f = open(file,&#39;r&#39;,encoding=&amp;quot;utf8&amp;quot;)
    review = f.read()
    data_test.append([review,1])
#appended values 0 and 1 represents negative and positive reviews about movies, respectively

data_test = pd.DataFrame(data_test)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_test.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;th&gt;1&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Once again Mr. Costner has dragged out a movie...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;I was looking forward to this movie. Trustwort...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;I gave this a 3 out of a possible 10 stars.&amp;lt;br...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;I of course saw the previews for this at the b...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;too bad this movie isn&#39;t. While &#34;Nemesis Game&#34;...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;
Converting the data frames to arrays for training and testing data &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train=data_train.iloc[:,0].values
Y_train=data_train.iloc[:,1].values
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_test=data_test.iloc[:,0].values
Y_test=data_test.iloc[:,1].values
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#print(X_train[3])
#print(X_test[30])
&lt;/code&gt;&lt;/pre&gt;
&lt;p &lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; Cleaning the X_train and X_test &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(len(X_train)):
    X_train[i]=Clean(X_train[i])
print(X_train[3])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;my girlfriend once brought around the zombie chronicles for us to watch as a joke little did we realize the joke was on her for paying 1 for it while watching this film i started to come up with things i would rather be doing than watching the zombie chronicles these included br br 1 drinking bleach 2 rubbing sand in my eyes 3 writing a letter to brad sykes and garrett clancy 4 re enacting the american civil war 5 tax returns 6 gcse maths 7 sex with an old lady br br garrett clancy aka sgt ben draper wrote this the guy couldn t even dig a hole properly the best ting he did was kick a door down the best part of the film this was the worst film i have ever seen and i ve seen white noise the light never has a film had so many mistakes in it my girlfriend left it here so now i live with the shame of owning this piece of crap br br news just in owen wilson watched this film and tried to kill himself fact br br do not watch
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for i in range(len(X_test)):
    X_test[i]=Clean(X_test[i])
#printX_test[30]
&lt;/code&gt;&lt;/pre&gt;
&lt;p &lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Diving the training data into training and development data &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import train_test_split
X_train,X_dev,Y_train,Y_dev=train_test_split(X_train,Y_train,test_size=0.2)
#x_train[10]
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Making the vocabulary for the data&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def build_freq_and_vocab(X):
    #vocab=set()
    vocab={}
    for val in X:
        revs=val.split()
        for word in revs:
            if word not in vocab:
                vocab[word]=1
            else:
                vocab[word]+=1
    return(vocab)

word_vocab=build_freq_and_vocab(X_train)
#len(word_vocab)
#word_vocab[&#39;idea&#39;]

less_than_five=[]
for i in word_vocab.keys():
    #print(i)
    if(word_vocab[i]&amp;lt;5):
         less_than_five.append(i)
#print(len(less_than_fove))
for i in less_than_five:
    del word_vocab[i]
#len(word_vocab)     
#X_train[0]
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Dispaying the vocabulary of words&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;word_vocab
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{&#39;in&#39;: 2939,
 &#39;today&#39;: 29,
 &#39;s&#39;: 1795,
 &#39;world&#39;: 129,
 &#39;of&#39;: 4371,
 &#39;digital&#39;: 5,
 &#39;there&#39;: 550,
 &#39;is&#39;: 3175,
 &#39;no&#39;: 370,
 &#39;computer&#39;: 18,
 &#39;than&#39;: 281,
 &#39;can&#39;: 446,
 &#39;the&#39;: 9911,
 &#39;actor&#39;: 90,
 &#39;and&#39;: 4979,
 &#39;writer&#39;: 29,
 &#39;alas&#39;: 9,
 &#39;this&#39;: 2326,
 &#39;type&#39;: 39,
 &#39;character&#39;: 222,
 &#39;driven&#39;: 7,
 &#39;film&#39;: 1258,
 &#39;far&#39;: 98,
 &#39;too&#39;: 235,
 &#39;rare&#39;: 11,
 &#39;these&#39;: 183,
 &#39;days&#39;: 45,
 &#39;duvall&#39;: 9,
 &#39;performance&#39;: 86,
 &#39;as&#39;: 1337,
 &#39;well&#39;: 282,
 &#39;james&#39;: 31,
 &#39;earl&#39;: 19,
 &#39;jones&#39;: 25,
 &#39;are&#39;: 919,
 &#39;to&#39;: 3989,
 &#39;their&#39;: 348,
 &#39;audience&#39;: 74,
 &#39;high&#39;: 68,
 &#39;expectations&#39;: 7,
 &#39;i&#39;: 2715,
 &#39;wonder&#39;: 31,
 &#39;if&#39;: 494,
 &#39;movie&#39;: 1238,
 &#39;was&#39;: 1457,
 &#39;made&#39;: 256,
 &#39;for&#39;: 1322,
 &#39;tv&#39;: 68,
 &#39;it&#39;: 2932,
 &#39;has&#39;: 497,
 &#39;a&#39;: 4792,
 &#39;close&#39;: 37,
 &#39;up&#39;: 364,
 &#39;personal&#39;: 25,
 &#39;quality&#39;: 42,
 &#39;narrative&#39;: 17,
 &#39;an&#39;: 681,
 &#39;say&#39;: 163,
 &#39;that&#39;: 2191,
 &#39;performances&#39;: 49,
 &#39;all&#39;: 737,
 &#39;outstanding&#39;: 12,
 &#39;only&#39;: 369,
 &#39;thing&#39;: 142,
 &#39;keeps&#39;: 15,
 &#39;from&#39;: 623,
 &#39;being&#39;: 210,
 &#39;cinema&#39;: 59,
 &#39;masterpiece&#39;: 27,
 &#39;lack&#39;: 27,
 &#39;great&#39;: 276,
 &#39;but&#39;: 1254,
 &#39;pretty&#39;: 88,
 &#39;pictures&#39;: 14,
 &#39;not&#39;: 885,
 &#39;everything&#39;: 74,
 &#39;how&#39;: 268,
 &#39;talent&#39;: 27,
 &#39;likes&#39;: 16,
 &#39;continue&#39;: 11,
 &#39;produce&#39;: 10,
 &#39;such&#39;: 157,
 &#39;fine&#39;: 49,
 &#39;work&#39;: 148,
 &#39;age&#39;: 38,
 &#39;where&#39;: 146,
 &#39;actors&#39;: 156,
 &#39;excellent&#39;: 71,
 &#39;with&#39;: 1268,
 &#39;extraordinary&#39;: 5,
 &#39;cast&#39;: 123,
 &#39;acting&#39;: 204,
 &#39;very&#39;: 431,
 &#39;disappointed&#39;: 26,
 &#39;academy&#39;: 16,
 &#39;when&#39;: 458,
 &#39;didn&#39;: 143,
 &#39;t&#39;: 1029,
 &#39;get&#39;: 265,
 &#39;oscar&#39;: 40,
 &#39;best&#39;: 192,
 &#39;actress&#39;: 64,
 &#39;goldberg&#39;: 24,
 &#39;certainly&#39;: 32,
 &#39;deserved&#39;: 14,
 &#39;any&#39;: 218,
 &#39;case&#39;: 40,
 &#39;take&#39;: 115,
 &#39;look&#39;: 102,
 &#39;at&#39;: 700,
 &#39;am&#39;: 86,
 &#39;sure&#39;: 78,
 &#39;you&#39;: 1025,
 &#39;will&#39;: 270,
 &#39;enjoy&#39;: 45,
 &#39;much&#39;: 292,
 &#39;films&#39;: 237,
 &#39;fill&#39;: 10,
 &#39;subject&#39;: 30,
 &#39;matter&#39;: 32,
 &#39;so&#39;: 635,
 &#39;after&#39;: 223,
 &#39;watching&#39;: 147,
 &#39;trailer&#39;: 7,
 &#39;water&#39;: 11,
 &#39;expected&#39;: 23,
 &#39;like&#39;: 583,
 &#39;because&#39;: 283,
 &#39;thought&#39;: 104,
 &#39;d&#39;: 88,
 &#39;on&#39;: 1058,
 &#39;something&#39;: 131,
 &#39;unique&#39;: 22,
 &#39;honestly&#39;: 16,
 &#39;portrays&#39;: 11,
 &#39;teen&#39;: 24,
 &#39;lesbian&#39;: 13,
 &#39;love&#39;: 273,
 &#39;sort&#39;: 32,
 &#39;female&#39;: 42,
 &#39;version&#39;: 51,
 &#39;beautiful&#39;: 77,
 &#39;br&#39;: 3152,
 &#39;main&#39;: 70,
 &#39;characters&#39;: 200,
 &#39;young&#39;: 114,
 &#39;french&#39;: 24,
 &#39;women&#39;: 95,
 &#39;15&#39;: 13,
 &#39;years&#39;: 143,
 &#39;old&#39;: 159,
 &#39;marie&#39;: 17,
 &#39;way&#39;: 240,
 &#39;floriane&#39;: 7,
 &#39;erotic&#39;: 8,
 &#39;between&#39;: 111,
 &#39;always&#39;: 85,
 &#39;surface&#39;: 11,
 &#39;then&#39;: 227,
 &#39;just&#39;: 550,
 &#39;below&#39;: 8,
 &#39;however&#39;: 98,
 &#39;about&#39;: 524,
 &#39;upon&#39;: 32,
 &#39;two&#39;: 243,
 &#39;sexual&#39;: 28,
 &#39;frustration&#39;: 5,
 &#39;suffering&#39;: 13,
 &#39;working&#39;: 34,
 &#39;cross&#39;: 6,
 &#39;purposes&#39;: 8,
 &#39;least&#39;: 99,
 &#39;sex&#39;: 63,
 &#39;also&#39;: 274,
 &#39;proves&#39;: 10,
 &#39;makers&#39;: 17,
 &#39;own&#39;: 92,
 &#39;they&#39;: 677,
 &#39;become&#39;: 53,
 &#39;extra&#39;: 8,
 &#39;features&#39;: 12,
 &#39;lord&#39;: 6,
 &#39;dvd&#39;: 71,
 &#39;director&#39;: 153,
 &#39;peter&#39;: 34,
 &#39;says&#39;: 34,
 &#39;cynicism&#39;: 5,
 &#39;starts&#39;: 35,
 &#39;meaning&#39;: 10,
 &#39;regard&#39;: 5,
 &#39;children&#39;: 41,
 &#39;while&#39;: 138,
 &#39;adults&#39;: 11,
 &#39;part&#39;: 125,
 &#39;maker&#39;: 6,
 &#39;joy&#39;: 20,
 &#39;asked&#39;: 11,
 &#39;myself&#39;: 42,
 &#39;yes&#39;: 43,
 &#39;first&#39;: 274,
 &#39;be&#39;: 850,
 &#39;painful&#39;: 14,
 &#39;fresh&#39;: 11,
 &#39;life&#39;: 230,
 &#39;positive&#39;: 18,
 &#39;aspects&#39;: 8,
 &#39;missing&#39;: 20,
 &#39;balance&#39;: 7,
 &#39;wants&#39;: 33,
 &#39;poignant&#39;: 6,
 &#39;celebration&#39;: 5,
 &#39;impressed&#39;: 11,
 &#39;her&#39;: 687,
 &#39;ruins&#39;: 7,
 &#39;what&#39;: 451,
 &#39;point&#39;: 77,
 &#39;showing&#39;: 17,
 &#39;girl&#39;: 81,
 &#39;nude&#39;: 9,
 &#39;know&#39;: 179,
 &#39;established&#39;: 5,
 &#39;tasteful&#39;: 5,
 &#39;nudity&#39;: 29,
 &#39;european&#39;: 8,
 &#39;by&#39;: 653,
 &#39;devil&#39;: 16,
 &#39;probably&#39;: 70,
 &#39;little&#39;: 178,
 &#39;heart&#39;: 41,
 &#39;friends&#39;: 52,
 &#39;europa&#39;: 57,
 &#39;instance&#39;: 5,
 &#39;see&#39;: 359,
 &#39;make&#39;: 217,
 &#39;show&#39;: 212,
 &#39;unattractive&#39;: 5,
 &#39;person&#39;: 52,
 &#39;either&#39;: 58,
 &#39;or&#39;: 487,
 &#39;would&#39;: 388,
 &#39;ve&#39;: 157,
 &#39;been&#39;: 288,
 &#39;honest&#39;: 16,
 &#39;go&#39;: 118,
 &#39;scene&#39;: 163,
 &#39;club&#39;: 13,
 &#39;dancing&#39;: 23,
 &#39;follows&#39;: 14,
 &#39;next&#39;: 51,
 &#39;perhaps&#39;: 72,
 &#39;biggest&#39;: 13,
 &#39;honesty&#39;: 7,
 &#39;takes&#39;: 63,
 &#39;place&#39;: 72,
 &#39;she&#39;: 524,
 &#39;normal&#39;: 11,
 &#39;asks&#39;: 13,
 &#39;who&#39;: 654,
 &#39;cares&#39;: 9,
 &#39;plays&#39;: 73,
 &#39;false&#39;: 9,
 &#39;question&#39;: 17,
 &#39;authentic&#39;: 10,
 &#39;heartfelt&#39;: 5,
 &#39;viewers&#39;: 35,
 &#39;time&#39;: 371,
 &#39;deserve&#39;: 16,
 &#39;here&#39;: 160,
 &#39;moments&#39;: 38,
 &#39;which&#39;: 319,
 &#39;viewer&#39;: 40,
 &#39;1&#39;: 55,
 &#39;since&#39;: 76,
 &#39;do&#39;: 251,
 &#39;people&#39;: 285,
 &#39;wearing&#39;: 6,
 &#39;suits&#39;: 6,
 &#39;turtle&#39;: 8,
 &#39;boot&#39;: 5,
 &#39;2&#39;: 89,
 &#39;down&#39;: 96,
 &#39;core&#39;: 5,
 &#39;thrown&#39;: 16,
 &#39;garbage&#39;: 13,
 &#39;order&#39;: 27,
 &#39;taste&#39;: 9,
 &#39;beloved&#39;: 5,
 &#39;mouth&#39;: 13,
 &#39;three&#39;: 80,
 &#39;actresses&#39;: 32,
 &#39;find&#39;: 129,
 &#39;better&#39;: 159,
 &#39;talents&#39;: 13,
 &#39;may&#39;: 92,
 &#39;terms&#39;: 18,
 &#39;technique&#39;: 7,
 &#39;could&#39;: 244,
 &#39;have&#39;: 850,
 &#39;successful&#39;: 17,
 &#39;career&#39;: 30,
 &#39;supporting&#39;: 30,
 &#39;roles&#39;: 47,
 &#39;leading&#39;: 20,
 &#39;lady&#39;: 35,
 &#39;both&#39;: 96,
 &#39;intensity&#39;: 10,
 &#39;future&#39;: 30,
 &#39;play&#39;: 65,
 &#39;emotionally&#39;: 7,
 &#39;rise&#39;: 6,
 &#39;disappointing&#39;: 16,
 &#39;please&#39;: 49,
 &#39;falling&#39;: 13,
 &#39;necessarily&#39;: 11,
 &#39;middle&#39;: 23,
 &#39;aged&#39;: 9,
 &#39;guy&#39;: 85,
 &#39;girls&#39;: 68,
 &#39;watch&#39;: 203,
 &#39;identify&#39;: 9,
 &#39;wonderful&#39;: 61,
 &#39;new&#39;: 127,
 &#39;crime&#39;: 27,
 &#39;series&#39;: 101,
 &#39;bringing&#39;: 7,
 &#39;together&#39;: 75,
 &#39;british&#39;: 17,
 &#39;television&#39;: 30,
 &#39;armstrong&#39;: 8,
 &#39;retired&#39;: 9,
 &#39;detectives&#39;: 9,
 &#39;brought&#39;: 32,
 &#39;back&#39;: 136,
 &#39;help&#39;: 36,
 &#39;clear&#39;: 23,
 &#39;cases&#39;: 6,
 &#39;under&#39;: 35,
 &#39;younger&#39;: 16,
 &#39;focused&#39;: 7,
 &#39;amanda&#39;: 6,
 &#39;quirky&#39;: 5,
 &#39;cops&#39;: 11,
 &#39;brilliant&#39;: 39,
 &#39;team&#39;: 29,
 &#39;twenty&#39;: 16,
 &#39;year&#39;: 87,
 &#39;police&#39;: 38,
 &#39;force&#39;: 8,
 &#39;moved&#39;: 20,
 &#39;long&#39;: 105,
 &#39;sometimes&#39;: 43,
 &#39;effect&#39;: 39,
 &#39;other&#39;: 284,
 &#39;times&#39;: 92,
 &#39;horror&#39;: 84,
 &#39;portrayed&#39;: 21,
 &#39;comic&#39;: 29,
 &#39;scenes&#39;: 142,
 &#39;some&#39;: 483,
 &#39;moving&#39;: 31,
 &#39;ones&#39;: 26,
 &#39;each&#39;: 82,
 &#39;come&#39;: 95,
 &#39;growing&#39;: 11,
 &#39;end&#39;: 155,
 &#39;six&#39;: 10,
 &#39;we&#39;: 301,
 &#39;further&#39;: 25,
 &#39;had&#39;: 330,
 &#39;developed&#39;: 11,
 &#39;cannot&#39;: 44,
 &#39;his&#39;: 783,
 &#39;wife&#39;: 51,
 &#39;death&#39;: 64,
 &#39;learning&#39;: 5,
 &#39;accept&#39;: 11,
 &#39;role&#39;: 94,
 &#39;grandfather&#39;: 15,
 &#39;even&#39;: 390,
 &#39;helped&#39;: 9,
 &#39;fight&#39;: 32,
 &#39;past&#39;: 41,
 &#39;keep&#39;: 54,
 &#39;taking&#39;: 37,
 &#39;face&#39;: 52,
 &#39;familiar&#39;: 29,
 &#39;conflict&#39;: 10,
 &#39;having&#39;: 64,
 &#39;story&#39;: 429,
 &#39;lines&#39;: 49,
 &#39;interesting&#39;: 118,
 &#39;rather&#39;: 85,
 &#39;heavily&#39;: 10,
 &#39;four&#39;: 40,
 &#39;finest&#39;: 9,
 &#39;failure&#39;: 8,
 &#39;box&#39;: 17,
 &#39;office&#39;: 9,
 &#39;b&#39;: 32,
 &#39;demille&#39;: 11,
 &#39;stopped&#39;: 7,
 &#39;doing&#39;: 55,
 &#39;non&#39;: 24,
 &#39;american&#39;: 82,
 &#39;history&#39;: 49,
 &#39;were&#39;: 357,
 &#39;our&#39;: 75,
 &#39;jean&#39;: 20,
 &#39;war&#39;: 62,
 &#39;ii&#39;: 6,
 &#39;dr&#39;: 8,
 &#39;production&#39;: 45,
 &#39;starring&#39;: 22,
 &#39;gary&#39;: 9,
 &#39;cooper&#39;: 17,
 &#39;wild&#39;: 22,
 &#39;bill&#39;: 26,
 &#39;hickok&#39;: 10,
 &#39;arthur&#39;: 14,
 &#39;calamity&#39;: 10,
 &#39;jane&#39;: 21,
 &#39;buffalo&#39;: 12,
 &#39;john&#39;: 56,
 &#39;villain&#39;: 11,
 &#39;usual&#39;: 20,
 &#39;general&#39;: 26,
 &#39;george&#39;: 43,
 &#39;custer&#39;: 7,
 &#39;one&#39;: 835,
 &#39;indians&#39;: 5,
 &#39;big&#39;: 117,
 &#39;villains&#39;: 7,
 &#39;led&#39;: 11,
 &#39;charles&#39;: 10,
 &#39;selling&#39;: 6,
 &#39;arms&#39;: 5,
 &#39;hall&#39;: 7,
 &#39;jack&#39;: 15,
 &#39;killed&#39;: 30,
 &#39;basically&#39;: 22,
 &#39;u&#39;: 16,
 &#39;civil&#39;: 8,
 &#39;lincoln&#39;: 9,
 &#39;shown&#39;: 44,
 &#39;start&#39;: 43,
 &#39;talking&#39;: 22,
 &#39;step&#39;: 10,
 &#39;now&#39;: 144,
 &#39;lee&#39;: 12,
 &#39;talks&#39;: 8,
 &#39;need&#39;: 53,
 &#39;west&#39;: 17,
 &#39;more&#39;: 485,
 &#39;later&#39;: 80,
 &#39;he&#39;: 828,
 &#39;theater&#39;: 17,
 &#39;april&#39;: 32,
 &#39;must&#39;: 91,
 &#39;busy&#39;: 24,
 &#39;city&#39;: 24,
 &#39;same&#39;: 133,
 &#39;date&#39;: 16,
 &#39;actually&#39;: 104,
 &#39;concerned&#39;: 12,
 &#39;immediate&#39;: 5,
 &#39;thoughts&#39;: 6,
 &#39;last&#39;: 93,
 &#39;day&#39;: 99,
 &#39;former&#39;: 20,
 &#39;states&#39;: 7,
 &#39;into&#39;: 260,
 &#39;union&#39;: 7,
 &#39;soon&#39;: 36,
 &#39;possible&#39;: 32,
 &#39;attention&#39;: 30,
 &#39;except&#39;: 22,
 &#39;problems&#39;: 25,
 &#39;forces&#39;: 12,
 &#39;mexico&#39;: 7,
 &#39;against&#39;: 41,
 &#39;involved&#39;: 25,
 &#39;actual&#39;: 24,
 &#39;sent&#39;: 9,
 &#39;loser&#39;: 5,
 &#39;second&#39;: 58,
 &#39;put&#39;: 83,
 &#39;serious&#39;: 40,
 &#39;indian&#39;: 6,
 &#39;novel&#39;: 29,
 &#39;lake&#39;: 18,
 &#39;jackson&#39;: 6,
 &#39;turned&#39;: 25,
 &#39;out&#39;: 466,
 &#39;quite&#39;: 99,
 &#39;effective&#39;: 14,
 &#39;western&#39;: 28,
 &#39;once&#39;: 78,
 &#39;should&#39;: 160,
 &#39;nothing&#39;: 133,
 &#39;said&#39;: 70,
 &#39;hardly&#39;: 19,
 &#39;profound&#39;: 6,
 &#39;saying&#39;: 21,
 &#39;eat&#39;: 5,
 &#39;good&#39;: 456,
 &#39;breakfast&#39;: 6,
 &#39;every&#39;: 158,
 &#39;morning&#39;: 5,
 &#39;your&#39;: 175,
 &#39;health&#39;: 6,
 &#39;statement&#39;: 7,
 &#39;fact&#39;: 116,
 &#39;turning&#39;: 9,
 &#39;minor&#39;: 10,
 &#39;ridiculous&#39;: 27,
 &#39;typical&#39;: 25,
 &#39;scripts&#39;: 7,
 &#39;really&#39;: 321,
 &#39;bad&#39;: 264,
 &#39;errors&#39;: 5,
 &#39;common&#39;: 20,
 &#39;sense&#39;: 66,
 &#39;them&#39;: 230,
 &#39;mistake&#39;: 13,
 &#39;adventure&#39;: 18,
 &#39;full&#39;: 44,
 &#39;creator&#39;: 6,
 &#39;worth&#39;: 73,
 &#39;political&#39;: 26,
 &#39;ideas&#39;: 18,
 &#39;study&#39;: 7,
 &#39;human&#39;: 53,
 &#39;nature&#39;: 24,
 &#39;context&#39;: 13,
 &#39;without&#39;: 87,
 &#39;simply&#39;: 48,
 &#39;everyone&#39;: 51,
 &#39;finished&#39;: 12,
 &#39;product&#39;: 6,
 &#39;performers&#39;: 9,
 &#39;worst&#39;: 86,
 &#39;sequel&#39;: 19,
 &#39;movies&#39;: 265,
 &#39;again&#39;: 138,
 &#39;doesn&#39;: 122,
 &#39;killer&#39;: 81,
 &#39;still&#39;: 149,
 &#39;kills&#39;: 28,
 &#39;fun&#39;: 57,
 &#39;killing&#39;: 26,
 &#39;making&#39;: 77,
 &#39;happened&#39;: 28,
 &#39;means&#39;: 22,
 &#39;ever&#39;: 202,
 &#39;don&#39;: 262,
 &#39;value&#39;: 16,
 &#39;hour&#39;: 33,
 &#39;during&#39;: 67,
 &#39;ll&#39;: 86,
 &#39;want&#39;: 104,
 &#39;ask&#39;: 17,
 &#39;him&#39;: 257,
 &#39;original&#39;: 94,
 &#39;makes&#39;: 131,
 &#39;action&#39;: 83,
 &#39;let&#39;: 54,
 &#39;child&#39;: 62,
 &#39;adult&#39;: 15,
 &#39;impact&#39;: 6,
 &#39;insult&#39;: 5,
 &#39;tmnt&#39;: 7,
 &#39;venus&#39;: 5,
 &#39;does&#39;: 151,
 &#39;never&#39;: 189,
 &#39;took&#39;: 37,
 &#39;away&#39;: 94,
 &#39;tragic&#39;: 10,
 &#39;tale&#39;: 29,
 &#39;4&#39;: 47,
 &#39;male&#39;: 23,
 &#39;family&#39;: 91,
 &#39;gone&#39;: 21,
 &#39;over&#39;: 212,
 &#39;power&#39;: 32,
 &#39;horrible&#39;: 34,
 &#39;episode&#39;: 34,
 &#39;voices&#39;: 7,
 &#39;wrong&#39;: 47,
 &#39;acted&#39;: 21,
 &#39;done&#39;: 100,
 &#39;job&#39;: 95,
 &#39;bother&#39;: 10,
 &#39;worthy&#39;: 9,
 &#39;material&#39;: 23,
 &#39;slow&#39;: 34,
 &#39;looking&#39;: 79,
 &#39;totally&#39;: 40,
 &#39;shredder&#39;: 6,
 &#39;dude&#39;: 5,
 &#39;corny&#39;: 7,
 &#39;turtles&#39;: 6,
 &#39;looked&#39;: 35,
 &#39;things&#39;: 95,
 &#39;hanging&#39;: 9,
 &#39;off&#39;: 170,
 &#39;bodies&#39;: 8,
 &#39;around&#39;: 83,
 &#39;silly&#39;: 24,
 &#39;got&#39;: 117,
 &#39;rid&#39;: 6,
 &#39;stupid&#39;: 59,
 &#39;cartoon&#39;: 8,
 &#39;sounds&#39;: 12,
 &#39;writing&#39;: 36,
 &#39;those&#39;: 120,
 &#39;kelly&#39;: 47,
 &#39;legend&#39;: 42,
 &#39;hoping&#39;: 12,
 &#39;accurate&#39;: 15,
 &#39;creative&#39;: 10,
 &#39;license&#39;: 6,
 &#39;taken&#39;: 44,
 &#39;naomi&#39;: 7,
 &#39;existed&#39;: 6,
 &#39;reality&#39;: 38,
 &#39;purely&#39;: 11,
 &#39;piece&#39;: 40,
 &#39;entertainment&#39;: 31,
 &#39;holds&#39;: 5,
 &#39;title&#39;: 45,
 &#39;solid&#39;: 20,
 &#39;ned&#39;: 57,
 &#39;hard&#39;: 85,
 &#39;considering&#39;: 19,
 &#39;previous&#39;: 15,
 &#39;mick&#39;: 5,
 &#39;jagger&#39;: 5,
 &#39;australian&#39;: 22,
 &#39;rules&#39;: 5,
 &#39;bob&#39;: 11,
 &#39;poor&#39;: 40,
 &#39;location&#39;: 11,
 &#39;shooting&#39;: 43,
 &#39;area&#39;: 12,
 &#39;live&#39;: 37,
 &#39;outside&#39;: 14,
 &#39;remember&#39;: 41,
 &#39;golden&#39;: 14,
 &#39;released&#39;: 35,
 &#39;critics&#39;: 10,
 &#39;m&#39;: 147,
 &#39;badly&#39;: 23,
 &#39;less&#39;: 63,
 &#39;ended&#39;: 11,
 &#39;eddie&#39;: 10,
 &#39;murphy&#39;: 13,
 &#39;guess&#39;: 38,
 &#39;going&#39;: 124,
 &#39;gets&#39;: 97,
 &#39;front&#39;: 20,
 &#39;blank&#39;: 19,
 &#39;expression&#39;: 10,
 &#39;guys&#39;: 35,
 &#39;enter&#39;: 9,
 &#39;sits&#39;: 6,
 &#39;pull&#39;: 6,
 &#39;giant&#39;: 8,
 &#39;cage&#39;: 12,
 &#39;stick&#39;: 14,
 &#39;inside&#39;: 21,
 &#39;impression&#39;: 19,
 &#39;michael&#39;: 29,
 &#39;give&#39;: 109,
 &#39;wooden&#39;: 8,
 &#39;sequence&#39;: 20,
 &#39;pop&#39;: 9,
 &#39;soundtrack&#39;: 29,
 &#39;obviously&#39;: 29,
 &#39;might&#39;: 103,
 &#39;cool&#39;: 27,
 &#39;seems&#39;: 126,
 &#39;completely&#39;: 52,
 &#39;somewhat&#39;: 26,
 &#39;bloody&#39;: 7,
 &#39;opening&#39;: 31,
 &#39;problem&#39;: 49,
 &#39;boy&#39;: 37,
 &#39;whole&#39;: 96,
 &#39;mood&#39;: 7,
 &#39;change&#39;: 24,
 &#39;different&#39;: 52,
 &#39;blame&#39;: 9,
 &#39;personally&#39;: 11,
 &#39;pointed&#39;: 6,
 &#39;producer&#39;: 15,
 &#39;share&#39;: 11,
 &#39;equal&#39;: 8,
 &#39;did&#39;: 158,
 &#39;anyone&#39;: 91,
 &#39;before&#39;: 126,
 &#39;fantasy&#39;: 33,
 &#39;martial&#39;: 11,
 &#39;arts&#39;: 10,
 &#39;comedy&#39;: 100,
 &#39;crap&#39;: 22,
 &#39;seeing&#39;: 57,
 &#39;used&#39;: 53,
 &#39;attend&#39;: 6,
 &#39;e&#39;: 16,
 &#39;almost&#39;: 94,
 &#39;forgotten&#39;: 17,
 &#39;me&#39;: 319,
 &#39;my&#39;: 339,
 &#39;wish&#39;: 30,
 &#39;tried&#39;: 21,
 &#39;recently&#39;: 18,
 &#39;discovered&#39;: 11,
 &#39;mind&#39;: 63,
 &#39;beyond&#39;: 28,
 &#39;understanding&#39;: 9,
 &#39;cat&#39;: 9,
 &#39;extremely&#39;: 34,
 &#39;truly&#39;: 59,
 &#39;message&#39;: 19,
 &#39;form&#39;: 23,
 &#39;content&#39;: 9,
 &#39;missed&#39;: 16,
 &#39;fan&#39;: 60,
 &#39;wave&#39;: 9,
 &#39;underground&#39;: 5,
 &#39;barely&#39;: 18,
 &#39;closing&#39;: 5,
 &#39;credits&#39;: 21,
 &#39;dropped&#39;: 5,
 &#39;art&#39;: 45,
 &#39;youth&#39;: 6,
 &#39;documentary&#39;: 26,
 &#39;playing&#39;: 58,
 &#39;its&#39;: 253,
 &#39;relationship&#39;: 31,
 &#39;real&#39;: 159,
 &#39;ways&#39;: 30,
 &#39;sight&#39;: 12,
 &#39;al&#39;: 37,
 &#39;cliver&#39;: 8,
 &#39;naked&#39;: 14,
 &#39;black&#39;: 92,
 &#39;nelson&#39;: 5,
 &#39;laura&#39;: 19,
 &#39;crawford&#39;: 11,
 &#39;ursula&#39;: 5,
 &#39;buchfellner&#39;: 8,
 &#39;kidnapped&#39;: 9,
 &#39;group&#39;: 24,
 &#39;ransom&#39;: 9,
 &#39;6&#39;: 29,
 &#39;million&#39;: 15,
 &#39;delivered&#39;: 8,
 &#39;island&#39;: 21,
 &#39;count&#39;: 15,
 &#39;vietnam&#39;: 5,
 &#39;hired&#39;: 7,
 &#39;save&#39;: 28,
 &#39;local&#39;: 8,
 &#39;tribe&#39;: 8,
 &#39;offer&#39;: 10,
 &#39;monster&#39;: 11,
 &#39;cannibal&#39;: 29,
 &#39;god&#39;: 30,
 &#39;eyes&#39;: 51,
 &#39;filming&#39;: 17,
 &#39;set&#39;: 61,
 &#39;cannibals&#39;: 8,
 &#39;bit&#39;: 91,
 &#39;comes&#39;: 82,
 &#39;thanks&#39;: 12,
 &#39;mostly&#39;: 34,
 &#39;hilarious&#39;: 25,
 &#39;track&#39;: 15,
 &#39;goofy&#39;: 5,
 &#39;franco&#39;: 17,
 &#39;split&#39;: 6,
 &#39;interview&#39;: 8,
 &#39;strong&#39;: 34,
 &#39;including&#39;: 29,
 &#39;whose&#39;: 29,
 &#39;most&#39;: 244,
 &#39;head&#39;: 47,
 &#39;seen&#39;: 206,
 &#39;trying&#39;: 60,
 &#39;tons&#39;: 5,
 &#39;gore&#39;: 23,
 &#39;paint&#39;: 8,
 &#39;variety&#39;: 8,
 &#39;slowly&#39;: 15,
 &#39;de&#39;: 17,
 &#39;waves&#39;: 16,
 &#39;sadly&#39;: 15,
 &#39;jess&#39;: 6,
 &#39;40&#39;: 12,
 &#39;minutes&#39;: 64,
 &#39;run&#39;: 29,
 &#39;80&#39;: 11,
 &#39;looks&#39;: 58,
 &#39;nice&#39;: 57,
 &#39;odd&#39;: 11,
 &#39;images&#39;: 18,
 &#39;darker&#39;: 6,
 &#39;dialog&#39;: 20,
 &#39;spanish&#39;: 14,
 &#39;listen&#39;: 8,
 &#39;gives&#39;: 51,
 &#39;16&#39;: 6,
 &#39;minute&#39;: 20,
 &#39;star&#39;: 94,
 &#39;spoilers&#39;: 26,
 &#39;wouldn&#39;: 30,
 &#39;hollow&#39;: 19,
 &#39;man&#39;: 205,
 &#39;commercial&#39;: 10,
 &#39;paul&#39;: 24,
 &#39;verhoeven&#39;: 25,
 &#39;kevin&#39;: 18,
 &#39;bacon&#39;: 32,
 &#39;elisabeth&#39;: 5,
 &#39;shue&#39;: 14,
 &#39;plus&#39;: 23,
 &#39;theme&#39;: 35,
 &#39;invisibility&#39;: 8,
 &#39;premise&#39;: 19,
 &#39;unfortunately&#39;: 38,
 &#39;week&#39;: 13,
 &#39;suspense&#39;: 15,
 &#39;predictable&#39;: 32,
 &#39;bunch&#39;: 13,
 &#39;scientists&#39;: 5,
 &#39;animals&#39;: 8,
 &#39;succeeded&#39;: 8,
 &#39;decides&#39;: 18,
 &#39;test&#39;: 8,
 &#39;himself&#39;: 60,
 &#39;invisible&#39;: 36,
 &#39;changes&#39;: 12,
 &#39;murder&#39;: 47,
 &#39;thin&#39;: 13,
 &#39;line&#39;: 68,
 &#39;ill&#39;: 9,
 &#39;suffers&#39;: 5,
 &#39;many&#39;: 207,
 &#39;special&#39;: 99,
 &#39;effects&#39;: 97,
 &#39;lead&#39;: 36,
 &#39;producers&#39;: 17,
 &#39;thriller&#39;: 23,
 &#39;giving&#39;: 28,
 &#39;damn&#39;: 8,
 &#39;admit&#39;: 17,
 &#39;fx&#39;: 6,
 &#39;awesome&#39;: 13,
 &#39;matrix&#39;: 7,
 &#39;enough&#39;: 97,
 &#39;directors&#39;: 17,
 &#39;care&#39;: 39,
 &#39;spectacular&#39;: 7,
 &#39;few&#39;: 95,
 &#39;perfectly&#39;: 14,
 &#39;fabulous&#39;: 11,
 &#39;plot&#39;: 212,
 &#39;starship&#39;: 5,
 &#39;troopers&#39;: 6,
 &#39;p&#39;: 9,
 &#39;reasons&#39;: 25,
 &#39;why&#39;: 122,
 &#39;joke&#39;: 19,
 &#39;woman&#39;: 103,
 &#39;won&#39;: 47,
 &#39;spoil&#39;: 7,
 &#39;moment&#39;: 30,
 &#39;okay&#39;: 15,
 &#39;went&#39;: 37,
 &#39;beginning&#39;: 41,
 &#39;die&#39;: 20,
 &#39;course&#39;: 77,
 &#39;dying&#39;: 5,
 &#39;low&#39;: 52,
 &#39;rule&#39;: 5,
 &#39;somebody&#39;: 12,
 &#39;alone&#39;: 36,
 &#39;lab&#39;: 13,
 &#39;perfect&#39;: 40,
 &#39;victim&#39;: 15,
 &#39;example&#39;: 42,
 &#39;ending&#39;: 58,
 &#39;absolutely&#39;: 51,
 &#39;hits&#39;: 10,
 &#39;falls&#39;: 27,
 &#39;ground&#39;: 10,
 &#39;leave&#39;: 22,
 &#39;quietly&#39;: 5,
 &#39;attacks&#39;: 5,
 &#39;kill&#39;: 33,
 &#39;screams&#39;: 6,
 &#39;heard&#39;: 38,
 &#39;explosion&#39;: 5,
 &#39;ago&#39;: 36,
 &#39;suddenly&#39;: 30,
 &#39;hear&#39;: 13,
 &#39;above&#39;: 23,
 &#39;mr&#39;: 26,
 &#39;supposed&#39;: 43,
 &#39;10&#39;: 126,
 &#39;awful&#39;: 50,
 &#39;theory&#39;: 16,
 &#39;directed&#39;: 23,
 &#39;chick&#39;: 15,
 &#39;o&#39;: 13,
 &#39;r&#39;: 11,
 &#39;g&#39;: 8,
 &#39;thousand&#39;: 6,
 &#39;picture&#39;: 67,
 &#39;sound&#39;: 48,
 &#39;rated&#39;: 16,
 &#39;c&#39;: 15,
 &#39;aren&#39;: 33,
 &#39;negative&#39;: 9,
 &#39;scores&#39;: 7,
 &#39;imdb&#39;: 24,
 &#39;com&#39;: 8,
 &#39;rating&#39;: 22,
 &#39;system&#39;: 12,
 &#39;ps&#39;: 7,
 &#39;called&#39;: 47,
 &#39;etc&#39;: 29,
 &#39;another&#39;: 137,
 &#39;warning&#39;: 17,
 &#39;following&#39;: 19,
 &#39;superb&#39;: 22,
 &#39;base&#39;: 6,
 &#39;events&#39;: 20,
 &#39;filmed&#39;: 27,
 &#39;presumably&#39;: 5,
 &#39;largely&#39;: 11,
 &#39;canadian&#39;: 13,
 &#39;crew&#39;: 21,
 &#39;caught&#39;: 19,
 &#39;half&#39;: 47,
 &#39;thoroughly&#39;: 12,
 &#39;blatant&#39;: 5,
 &#39;historical&#39;: 37,
 &#39;propaganda&#39;: 6,
 &#39;susan&#39;: 6,
 &#39;sarandon&#39;: 7,
 &#39;assume&#39;: 6,
 &#39;born&#39;: 8,
 &#39;heroes&#39;: 11,
 &#39;small&#39;: 44,
 &#39;private&#39;: 9,
 &#39;based&#39;: 37,
 &#39;air&#39;: 13,
 &#39;dislike&#39;: 6,
 &#39;neil&#39;: 16,
 &#39;simon&#39;: 22,
 &#39;among&#39;: 28,
 &#39;entertaining&#39;: 33,
 &#39;comedies&#39;: 23,
 &#39;watched&#39;: 60,
 &#39;connection&#39;: 7,
 &#39;meet&#39;: 26,
 &#39;ah&#39;: 6,
 &#39;afraid&#39;: 14,
 &#39;changed&#39;: 20,
 &#39;men&#39;: 52,
 &#39;dull&#39;: 19,
 &#39;stars&#39;: 54,
 &#39;review&#39;: 17,
 &#39;living&#39;: 41,
 &#39;grandmother&#39;: 5,
 &#39;walter&#39;: 32,
 &#39;matthau&#39;: 63,
 &#39;magnificent&#39;: 12,
 &#39;secondly&#39;: 9,
 &#39;burns&#39;: 62,
 &#39;somehow&#39;: 18,
 &#39;late&#39;: 42,
 &#39;enjoyed&#39;: 41,
 &#39;although&#39;: 73,
 &#39;recognize&#39;: 9,
 &#39;remarkable&#39;: 9,
 &#39;top&#39;: 61,
 &#39;laugh&#39;: 37,
 &#39;turn&#39;: 39,
 &#39;pleasure&#39;: 13,
 &#39;roughly&#39;: 5,
 &#39;knew&#39;: 31,
 &#39;nazi&#39;: 14,
 &#39;everybody&#39;: 24,
 &#39;speaking&#39;: 12,
 &#39;danish&#39;: 16,
 &#39;decided&#39;: 21,
 &#39;check&#39;: 40,
 ...}
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Removing the words that occur less than five times&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def occurence_less_than_five(data,occurence):
    for i,v in enumerate(data):
        new_vocab=[]
        for line in v.split():
            if line in occurence.keys():
                new_vocab.append(line)
        new_vocab=&#39; &#39;.join(new_vocab)
        data[i]=new_vocab
    return data
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_train=occurence_less_than_five(X_train,word_vocab)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#X_train[0]
after_remove_word_vocab=build_freq_and_vocab(X_train)
print(&amp;quot;Length of word vocab after removing occurence less than 5\n&amp;quot;len(after_remove_word_vocab))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Length of word vocab after removing occurence less than 5
26300
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def Word_Prob(word):
    word_rev = 0
    total_rev = 0
    for class_label in word:
        total_rev= total_rev+len(word_vocab[word])           
        for rev in class_label[word]:
            if word in rev:
                word_rev=+1
    prob = word_rev/total_rev 
    return prob
#to calculate number of documents that has a particular word
def word_in_doc(X,word_vocab):
    vocab_list={}
    for word in word_vocab.keys():
        count=0
        for line in X:
            if word in line:
                count+=1
        vocab_list[word]=count
    return vocab_list
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;word_in_document=word_in_doc(X_train,word_vocab)
#len(word_in_document)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#P[the]=# of doc containing the/total no . of doc
def denom_prob_count(datab,word):
    if word not in word_in_document.keys():
        count=0
    else:
        count=word_in_document[word]
    return count/len(X_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Probability(the)=no.of docs containing &#39;the&#39;/total documents&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;denom=denom_prob_count(X_train,&#39;the&#39;)
print(&amp;quot;P[the]=No.of documents containing &#39;the&#39;/ Total number of documents&amp;quot;)
print(denom)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;P[the]=No.of documents containing &#39;the&#39;/ Total number of documents
0.9962546816479401
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
def word_in_doc_count(X,Y,occurence,sent):
    word_pos_count={}
    word_neg_count={}
    
    if sent==&#39;positive&#39;:
        for word in occurence.keys():
            cn=0
            for i,rev in enumerate(X):
                if word in rev.split() and Y[i]==1:
                    cn+=1
            word_pos_count[word]=cn
        return word_pos_count

    if sent==&#39;negative&#39;:
        for word in occurence.keys():
            cn=0
            for i,rev in enumerate(X):
                if word in rev.split() and Y[i]==0:
                    cn+=1
            word_neg_count[word]=cn
        return word_neg_count

    
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;wordneg=word_in_doc_count(X_train,Y_train,word_vocab,sent=&amp;quot;negative&amp;quot;)
wordpos=word_in_doc_count(X_train,Y_train,word_vocab,sent=&amp;quot;positive&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#total length of positive and negative documents

def prob_document_length(Y_train):
    len_pos=0
    len_neg=0
    for i in range(len(Y_train)):
        if(Y_train[i]==1):
            len_pos+=1
        else:
            len_neg+=1
    return(len_pos),(len_neg)

len_pos,len_neg=prob_document_length(Y_train)
#print(len(Y_train))
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Finding P(the|pos) or probability or word given a sentiment&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#number of the word in positive or negative document
def numerat(word,sent):
    if sent==&#39;positive&#39;:
        if word not in wordpos.keys():
            count=0
        else:
            count=wordpos[word]
            
    elif sent==&#39;negative&#39;:
        if word not in wordneg.keys():
            count=0
        else:
            count=wordneg[word]

    return count

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;##eg: P(the|positive)

numerator=numerat(&#39;the&#39;,sent=&#39;positive&#39;)
prob=numerator/len_pos
print(prob)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.9925
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Building the naive bayes model&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def NB(X_train,Y_train,word,flag_smooth,sent):
    #naiv_prob=0
    if sent==&#39;positive&#39;:
        
        num1=(numerat(word,sent))/len_pos
        num2=len_pos/len(Y_train)
        a=(num1*num2)
        den=denom_prob_count(X_train,word)
        den_actual=den if den!=0 else 0.001
        if flag_smooth==True:
            a_new=a+1
            
            d_new=den_actual
            naiv_prob=a_new/float(d_new)   
        else:
            naiv_prob=a/float(den_actual) 
    
    elif sent==&#39;negative&#39;:
        num1=(numerat(word,sent))/len_neg
        num2=len_neg/len(Y_train)
        a=(num1*num2)
        den=denom_prob_count(X_train,word) 
        den_actual=den if den!=0 else 0.001
        if flag_smooth==True:
            a_new=a+1
            d_new=den_actual+2
            naiv_prob=a_new/float(d_new)
        else:
            naiv_prob=a/float(den_actual)
    #print(naiv_prob)        
    return naiv_prob
    
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#example of naive bayes

r=NB(X_train,Y_train,&#39;the&#39;,True,sent=&#39;positive&#39;)
print(r)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.49916666666666665
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def NB_all(X_train,Y_train,test,flag_smooth):
    sum1=1
    sum2=1
    y_pred=[]
    for i,v in enumerate(test):
        for word in v.split():
            sum1=(sum1*NB(X_train,Y_train,word,flag_smooth,sent=&amp;quot;positive&amp;quot;))
            sum2=(sum2*NB(X_train,Y_train,word,flag_smooth,sent=&amp;quot;negative&amp;quot;,))
        if(sum1&amp;lt;sum2):
            y_pred.append(0)
            #print(&amp;quot;negative&amp;quot;)
        else:
            y_pred.append(1)
            #print(&amp;quot;positive&amp;quot;)
        #print(&amp;quot;sum&amp;quot;,sum1,sum2)
    return(y_pred)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def accuracy_score(actual,predicted):
    count=0
    for i in range(len(actual)):
        if(actual[i]==predicted[i]):
            count+=1

    return count/float(len(actual))
        
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:16px;text-align:center;&gt;Calculating the accuracy of the development dataset&lt;/p&gt;
&lt;p style=&#34;font-family:Georgia;font-size:15.3px;text-align:center;&#34;&gt;As we can see we can make improvements if we tune our hyperparameters&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#print(pred)
pred=NB_all(X_train,Y_train,X_dev,flag_smooth=False)
score=accuracy_score(Y_dev,pred)
print(&amp;quot;Accuracy of the development data set without smoothing &amp;quot;,(score*100))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy of the development data set without smoothing  50.24875621890548
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;len_pos,len_neg=prob_document_length(Y_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Deriving the top ten positive words&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import operator
pos={}
neg={}
#root=[]
for word in word_vocab.keys():
    top_pos=NB(X_train,Y_train,word,False,sent=&#39;positive&#39;)
    top_neg=NB(X_train,Y_train,word,False,sent=&#39;negative&#39;)
    pos[word]=top_pos
    neg[word]=top_neg
#Reference:https://stackoverflow.com/questions/7197315/5-maximum-values-in-a-python-dictionary
top_pos_10=dict(sorted(pos.items(),key=operator.itemgetter(1), reverse=True)[:10])
top_neg_10=dict(sorted(neg.items(),key=operator.itemgetter(1), reverse=False)[:10])

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;top 10 positive words::&amp;quot;)
for word in top_pos_10.keys():
    print(word)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;top 10 positive words::
custer
tsui
depicted
georgia
patricia
plots
willy
alicia
resort
religion
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&amp;quot;top 10 negative words::&amp;quot;)
for word in top_neg_10.keys():
    print(word)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;top 10 negative words::
goldberg
retired
finest
demille
ii
custer
lincoln
busy
immediate
attend
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Comparing the effects of smoothing&lt;/p&gt;
&lt;p style=&#34;font-family:Georgia;font-size:15.3px;&#34;&gt;Considering the optimal hyperparameter , smoothing increases the accuracy of our model&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pred=NB_all(X_train,Y_train,X_test,flag_smooth=False)
score=accuracy_score(Y_test,pred)
#pred=NB_all(X_train,Y_train,X_train,flag_smooth=True)
#score=accuracy_score(Y_train,pred)
print(score*100)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;57.496
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Using the test dataset&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pred=NB_all(X_train,Y_train,X_test,flag_smooth=True)
score=accuracy_score(Y_train,pred)
#pred=NB_all(X_train,Y_train,X_test,flag_smooth=False)
#score=accuracy_score(Y_test,pred)
print(&amp;quot;Probabiity of the dataset after optimal hyperparameters and smoothing \n&amp;quot;,score*100)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Probabiity of the test dataset after optimal hyperparameters and smoothing
64.763
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=font-weight:bold;&gt;We have seen that Naive Bayes can be used extensively for text categorization and solving the problem or whether a document belongs to one sentiment or another.Some problems faced during the implemetation of this assignment were targeting a large dataset.&lt;br /&gt;We can see that the smoothing increases the accuracy slightly and we can compile a better probabilistic model if smoothing(eg.Laplace) is done.To further improve accuracy we can use some tactics like vectorisation or lemmatizing our words.Naive Bayes as a whole certainly aids to classify a text dataset in those aspects&lt;/p&gt;
&lt;p&gt;References:
&lt;a href=&#34;https://towardsdatascience.com/unfolding-na%C3%AFve-bayes-from-scratch-2e86dcae4b01#08ef&#34;&gt;https://towardsdatascience.com/unfolding-na%C3%AFve-bayes-from-scratch-2e86dcae4b01#08ef&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/&#34;&gt;https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://machinelearningmastery.com/load-machine-learning-data-scratch-python/&#34;&gt;https://machinelearningmastery.com/load-machine-learning-data-scratch-python/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://machinelearningmastery.com/implement-resampling-methods-scratch-python/&#34;&gt;https://machinelearningmastery.com/implement-resampling-methods-scratch-python/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Twitter Real or Not prediction</title>
      <link>https://sushantmhambrey.github.io/post/twitter/</link>
      <pubDate>Mon, 10 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://sushantmhambrey.github.io/post/twitter/</guid>
      <description>&lt;p&gt;Importing the necessary libraries&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np 
import pandas as pd 
import os
import seaborn as sns
import string
import csv
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import linear_model
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Loading the training and test data using pandas library&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.model_selection import train_test_split
train = pd.read_csv(&#39;../input/nlp-getting-started/train.csv&#39;)
test = pd.read_csv(&#39;../input/nlp-getting-started/test.csv&#39;)
submit = pd.read_csv(&amp;quot;/kaggle/input/nlp-getting-started/sample_submission.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reading the first five values of the training dataset&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;id&lt;/th&gt;
      &lt;th&gt;keyword&lt;/th&gt;
      &lt;th&gt;location&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
      &lt;th&gt;target&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Our Deeds are the Reason of this #earthquake M...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Forest fire near La Ronge Sask. Canada&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;All residents asked to &#39;shelter in place&#39; are ...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;13,000 people receive #wildfires evacuation or...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;Just got sent this photo from Ruby #Alaska as ...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print (train.shape, test.shape, submit.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(7613, 5) (3263, 4) (3263, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the count of the null values&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(train.isnull().sum())
print(test.isnull().sum())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;id             0
keyword       61
location    2533
text           0
target         0
dtype: int64
id             0
keyword       26
location    1105
text           0
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt
sns.countplot(y=train.target)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7fbc0ccb6cc0&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./twitter_9_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Looking at some examples of training before cleaning the text&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train[&#39;text&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0       Our Deeds are the Reason of this #earthquake M...
1                  Forest fire near La Ronge Sask. Canada
2       All residents asked to &#39;shelter in place&#39; are ...
3       13,000 people receive #wildfires evacuation or...
4       Just got sent this photo from Ruby #Alaska as ...
                              ...                        
7608    Two giant cranes holding a bridge collapse int...
7609    @aria_ahrary @TheTawniest The out of control w...
7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...
7611    Police investigating after an e-bike collided ...
7612    The Latest: More Homes Razed by Northern Calif...
Name: text, Length: 7613, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Defining a function to clean the dataset&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import re
def clean(text):
    text=re.sub(r&#39;https?://\S+&#39;, &#39;&#39;, text)
    text=re.sub(r&#39;&amp;lt;.*?&amp;gt;&#39;,&#39;&#39;,text) 
    text=re.sub(r&#39;\n&#39;,&#39; &#39;, text)
    text=re.sub(&#39;\s+&#39;, &#39; &#39;, text).strip()
    return text
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train[&#39;text&#39;] = train[&#39;text&#39;].apply(lambda x : clean(x))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the examples after cleaning the dataset&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train[&#39;text&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0       Our Deeds are the Reason of this #earthquake M...
1                  Forest fire near La Ronge Sask. Canada
2       All residents asked to &#39;shelter in place&#39; are ...
3       13,000 people receive #wildfires evacuation or...
4       Just got sent this photo from Ruby #Alaska as ...
                              ...                        
7608    Two giant cranes holding a bridge collapse int...
7609    @aria_ahrary @TheTawniest The out of control w...
7610           M1.94 [01:04 UTC]?5km S of Volcano Hawaii.
7611    Police investigating after an e-bike collided ...
7612    The Latest: More Homes Razed by Northern Calif...
Name: text, Length: 7613, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def remove_emoji(text):
    emoji_pattern = re.compile(&amp;quot;[&amp;quot;
                           u&amp;quot;\U0001F600-\U0001F64F&amp;quot;  # emoticons
                           u&amp;quot;\U0001F300-\U0001F5FF&amp;quot;  # symbols &amp;amp; pictographs
                           u&amp;quot;\U0001F680-\U0001F6FF&amp;quot;  # transport &amp;amp; map symbols
                           u&amp;quot;\U0001F1E0-\U0001F1FF&amp;quot;  # flags (iOS)
                           u&amp;quot;\U00002702-\U000027B0&amp;quot;
                           u&amp;quot;\U000024C2-\U0001F251&amp;quot;
                           &amp;quot;]+&amp;quot;, flags=re.UNICODE)
    return emoji_pattern.sub(r&#39;&#39;, text)

train[&#39;text&#39;] = train[&#39;text&#39;].apply(lambda x: remove_emoji(x))


# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train[&#39;text&#39;]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0       Our Deeds are the Reason of this #earthquake M...
1                  Forest fire near La Ronge Sask. Canada
2       All residents asked to &#39;shelter in place&#39; are ...
3       13,000 people receive #wildfires evacuation or...
4       Just got sent this photo from Ruby #Alaska as ...
                              ...                        
7608    Two giant cranes holding a bridge collapse int...
7609    @aria_ahrary @TheTawniest The out of control w...
7610           M1.94 [01:04 UTC]?5km S of Volcano Hawaii.
7611    Police investigating after an e-bike collided ...
7612    The Latest: More Homes Razed by Northern Calif...
Name: text, Length: 7613, dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def remove_punct(text):
    table=str.maketrans(&#39;&#39;,&#39;&#39;,string.punctuation)
    return text.translate(table)

#train_data[&#39;text&#39;] = train_data[&#39;text&#39;].apply(lambda x : remove_punct(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Assigning x to our text and y to our target column&lt;/p&gt;
&lt;p&gt;Splitting the training and testing dataset/&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x = train[&amp;quot;text&amp;quot;]
y = train[&amp;quot;target&amp;quot;]
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to vectorize the training dataset examples&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;vectorize = CountVectorizer(stop_words = &#39;english&#39;)
x_vector_train = vectorize.fit_transform(X_train)
x_vector_test = vectorize.transform(X_test)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our first model is the multinomial Naive Bayes model. Using the fit and predict methods and calculating the accuracy&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = MultinomialNB()
model.fit(x_vector_train, y_train)
prediction = model.predict(x_test_cv)
acc=accuracy_score(y_test,prediction)
print(acc)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.7892317793827971
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will test one more model called as Ridge Classifier and printing the accuracy&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;clf =  linear_model.RidgeClassifier()
clf.fit(x_vector_train, y_train)
prediction1=clf.predict(x_test_cv)
acc1=accuracy_score(y_test,prediction1)
print(acc1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.768220617202889
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see Multinomial Naive Bayes model performs a little better than Naive Bayes model by almost 2 percentage. Thus we will use that model to make our submission file&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#predicting on the test values
x_test=test[&amp;quot;text&amp;quot;]

#vectorizing the data
x_test_vector=vectorize.transform(x_test)

#making predictions
prediction=model.predict(x_test_vector)

#making submission
submit[&amp;quot;target&amp;quot;]=prediction

print(submit.head(10))



&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;   id  target
0   0       1
1   2       1
2   3       1
3   9       1
4  11       1
5  12       1
6  21       0
7  22       0
8  27       0
9  29       0
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;submit.to_csv(&amp;quot;submission.csv&amp;quot;,index=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Implementing kNN from scratch on IRIS dataset</title>
      <link>https://sushantmhambrey.github.io/post/assn2/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://sushantmhambrey.github.io/post/assn2/</guid>
      <description>&lt;p style=background-color:#ff7814;font-weight:bold;font-size:16px;text-align:center;&gt; Introduction to kNN&lt;/p&gt;
&lt;p style=&#34;font-family:Georgia;font-size:16px;&#34;&gt;The K-nearest-neighbor (kNN) is one of the most important and simple methods which can be used for both classification and regression problems but is more widely preferred in classification. Although it is simplistic in nature, the KNN algorithm can have better performance levels than many other classifiers’ is usually referred to as a “lazy, non parametric” learning algorithm. A non-parametric technique usually means that it does not assume anything about the data distribution. The structure of the model is defined by the data which is very advantageous when viewed from real world perspective. For these reasons , the rudimentary kNN algorithm can be considered as a good starting point for classification problems containing little or no prior knowledge about distribution data.
One of the most important test cases of kNN can be determining similarity of documents(sometimes referred to as “Concept Search”).Though kNN is easy to use and understand it has its own downfalls.As compared to neural network or SVM , kNN performs really slowly and can sometimes be less accurate.&lt;/p&gt;
&lt;p style=&#34;font-family:Georgia;font-size:15.3px;&#34;&gt;The kNN working is really simple. There is minimal training and heavy testing involved. When we need to make a prediction, the k-most similar neighbors are located and an equivalent prediction is made. It is like forming a “majority vote” between the k most similar instances to a new unobserved instance. Similarity is the distance metric between two data points. There are a number of distance measures available each with better accuracy than other depending on the given use cases and the hyperparameter selections. Usually a good starting point of distance measure in case of tabular data is the “Euclidian distance”&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Exporting the necessary libraries&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np 
import pandas as pd
import seaborn as sns
from collections import Counter
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=&#34;font-family:Georgia;font-size:15px;&#34;&gt;The assignment is about leveraging kNN in Python on a simple classification problem.The dataset at hand is the “Iris Flower Dataset(IFD)”  taken from UC Irvine Machine Learning Repository. The set has 3 Iris species((Iris setosa, Iris virginica and Iris versicolor) each having 50 observations. We have 4 features(attributes): 2 length(sepal_length,petal_length) and 2 width(sepal_width,petal_width)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris = pd.read_csv(&amp;quot;iris.csv&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; Observing the data. Iris dataset consists of four features and one class attribute&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.head()

&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sepal_length&lt;/th&gt;
      &lt;th&gt;sepal_width&lt;/th&gt;
      &lt;th&gt;petal_length&lt;/th&gt;
      &lt;th&gt;petal_width&lt;/th&gt;
      &lt;th&gt;class&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5.1&lt;/td&gt;
      &lt;td&gt;3.5&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;Iris-setosa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;4.9&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;Iris-setosa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4.7&lt;/td&gt;
      &lt;td&gt;3.2&lt;/td&gt;
      &lt;td&gt;1.3&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;Iris-setosa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;4.6&lt;/td&gt;
      &lt;td&gt;3.1&lt;/td&gt;
      &lt;td&gt;1.5&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;Iris-setosa&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;3.6&lt;/td&gt;
      &lt;td&gt;1.4&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;Iris-setosa&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.describe()

&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
&lt;pre&gt;&lt;code&gt;.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/style&gt;&lt;/p&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;sepal_length&lt;/th&gt;
      &lt;th&gt;sepal_width&lt;/th&gt;
      &lt;th&gt;petal_length&lt;/th&gt;
      &lt;th&gt;petal_width&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;count&lt;/td&gt;
      &lt;td&gt;150.000000&lt;/td&gt;
      &lt;td&gt;150.000000&lt;/td&gt;
      &lt;td&gt;150.000000&lt;/td&gt;
      &lt;td&gt;150.000000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mean&lt;/td&gt;
      &lt;td&gt;5.843333&lt;/td&gt;
      &lt;td&gt;3.054000&lt;/td&gt;
      &lt;td&gt;3.758667&lt;/td&gt;
      &lt;td&gt;1.198667&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;std&lt;/td&gt;
      &lt;td&gt;0.828066&lt;/td&gt;
      &lt;td&gt;0.433594&lt;/td&gt;
      &lt;td&gt;1.764420&lt;/td&gt;
      &lt;td&gt;0.763161&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;min&lt;/td&gt;
      &lt;td&gt;4.300000&lt;/td&gt;
      &lt;td&gt;2.000000&lt;/td&gt;
      &lt;td&gt;1.000000&lt;/td&gt;
      &lt;td&gt;0.100000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;25%&lt;/td&gt;
      &lt;td&gt;5.100000&lt;/td&gt;
      &lt;td&gt;2.800000&lt;/td&gt;
      &lt;td&gt;1.600000&lt;/td&gt;
      &lt;td&gt;0.300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;50%&lt;/td&gt;
      &lt;td&gt;5.800000&lt;/td&gt;
      &lt;td&gt;3.000000&lt;/td&gt;
      &lt;td&gt;4.350000&lt;/td&gt;
      &lt;td&gt;1.300000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;75%&lt;/td&gt;
      &lt;td&gt;6.400000&lt;/td&gt;
      &lt;td&gt;3.300000&lt;/td&gt;
      &lt;td&gt;5.100000&lt;/td&gt;
      &lt;td&gt;1.800000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;max&lt;/td&gt;
      &lt;td&gt;7.900000&lt;/td&gt;
      &lt;td&gt;4.400000&lt;/td&gt;
      &lt;td&gt;6.900000&lt;/td&gt;
      &lt;td&gt;2.500000&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; Analysing the data and plotting various iris classes in swarmplot.&lt;/p&gt;
&lt;p style=font-weight:bold;&gt; From the plot it can be observed that iris setosa has the least length range while virginica has the longest range&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sns.swarmplot(x=&#39;class&#39;,y=&#39;sepal_length&#39;,data=iris)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x16d2639ac88&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_13_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;iris.groupby(&#39;class&#39;).size()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;class
Iris-setosa        50
Iris-versicolor    50
Iris-virginica     50
dtype: int64
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.preprocessing import LabelEncoder
feature_columns = [&#39;sepal_length&#39;, &#39;sepal_width&#39;, &#39;petal_length&#39;,&#39;petal_width&#39;]
X = np.array(iris[feature_columns])
y = np.array(iris[&#39;class&#39;])
np.size(y)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;150
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The last column of the dataset represents the class values of the corresponding data.We need to convert this into integer for prediction&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;last_column= LabelEncoder()
y = last_column.fit_transform(y)
y
from sklearn.model_selection import train_test_split
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=font-weight:bold;&gt;We can observe that the classes are assigned integer values respectively. Example:: Iris-Setosa becomes 0, Versicolor becomes 1 and Virginica becomes 2 &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size = 0.35)
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; Writing kNN fit from scratch &lt;/p&gt;
&lt;p style=font-weight:bold;&gt; Here is where we implement the actual magic. In a nutshell , a particular row is measure against the rest of the development set and a majority vote is returned.As we have seen earlier kNN is a Lazy algorithm , it doesnt require an implementation of a train function--which is just learning the data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from numpy import dot
from numpy.linalg import norm

#----------EUCLIDIAN DISTANCE------------------------
def My_knn_fit(X_dev, y_dev, x_test, k,measure):
    if(measure==&amp;quot;euc&amp;quot;):
        dist_euc = []
        class_output = []

        for i in range(len(X_dev)):
            dist_euc.append([np.sqrt(np.sum(np.square(x_test - X_dev[i, :]))),i])
            
        dist_euc_sort = sorted(dist_euc)

        for i in range(k):
            class_in = dist_euc_sort[i][1]
            class_output.append(y_dev[class_in])
        majority_vote=Counter(class_output).most_common(1)[0][0]
        
        return majority_vote
    
#------------COSINE DISTANCE---------------------------    
    elif(measure==&amp;quot;cosine&amp;quot;):
        dist_cos = []
        class_output = []

        for i in range(len(X_dev)):
            cos_sim = np.sum((dot(x_test,X_dev[i, :]))/(norm(X_dev[i, :])*norm(x_test)))
            cos_dis=(1-cos_sim)
            dist_cos.append([(cos_dis),i])
            
        dist_cos_sort = sorted(dist_cos)
        
        for i in range(k):
            class_in = dist_cos_sort[i][1]
            class_output.append(y_dev[class_in])
        majority_vote=Counter(class_output).most_common(1)[0][0]   
        return majority_vote
    
#------------NORMALIZED-EUCLIDIAN DISTANCE-------------
    elif(measure==&amp;quot;norm_euc&amp;quot;):     
    #0.5*var(X-Y)/var(x)-var(y)
        dist_norm = []
        class_output = []
        for i in range(len(X_dev)):
            dist_norm.append([np.sum((np.var(X_dev[i, :]-x_test)/(np.var(X_dev[i,:]-np.var(x_test))))),i])
    
        dist_norm_sort = sorted(dist_norm)
        for i in range(k):
            class_in = dist_norm_sort[i][1]
            class_output.append(y_dev[class_in])
        majority_vote=Counter(class_output).most_common(1)[0][0]     
        return majority_vote

        
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Implementing kNN predict function &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def My_KNN_predict(X_dev, y_dev, X_test, k):

    pred_euc = []
    pred_cosine=[]
    pred_norm=[]
    for i in range(len(X_test)):
        pred_euc.append(My_knn_fit(X_dev, y_dev, X_test[i, :], k,&amp;quot;euc&amp;quot;))
        pred_cosine.append(My_knn_fit(X_dev, y_dev, X_test[i, :], k,&amp;quot;cosine&amp;quot;))
        pred_norm.append(My_knn_fit(X_dev, y_dev, X_test[i, :], k,&amp;quot;norm_euc&amp;quot;))
    pred_euc=np.array(pred_euc)
    pred_cosine=np.array(pred_cosine)
    pred_norm=np.array(pred_norm)
    #print(&amp;quot;cos:&amp;quot;,pred_cosine)
    #print(&amp;quot;norm:&amp;quot;,pred_norm)
    return (pred_euc,pred_cosine,pred_norm)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; Let&#39;s try out a random input taken from the csv file to see if the predcition is done correctly.The random input taken belong to setosa which corresponds to class 0 &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ran=[5.1,3.5,1.4,0.2]
predin=My_knn_fit(X_dev,y_dev,ran,5,&amp;quot;euc&amp;quot;)
if(predin==0):
    print(&amp;quot;iris-setosa&amp;quot;)
elif(predin==1):
    print(&amp;quot;iris-versicolor&amp;quot;)
else:
    print(&amp;quot;iris-virginica&amp;quot;)

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;iris-setosa
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def accuracy_scor(y_test, prediction):
    correct = 0
    n = len(y_test)
    for i in range(n):
        if(y_test[i]==prediction[i]):
            correct+=1
    accuracy_sc = (correct*100)/n
    return accuracy_sc
acc_cosine=[]
acc_euc=[]
acc_norm=[]
for k in range(1,8,2):
    pred_euc,pred_cosine,pred_norm = My_KNN_predict(X_dev, y_dev, X_test,k)
    # evaluating accuracy
    accuracy1 = accuracy_scor(y_test, pred_euc)
    acc_euc.append(accuracy1)
    accuracy2=accuracy_scor(y_test, pred_cosine)
    acc_cosine.append(accuracy2)
    accuracy3=accuracy_scor(y_test, pred_norm)
    acc_norm.append(accuracy3)
print(&amp;quot;Euclidian accuracy:&amp;quot;,(acc_euc))
print(&amp;quot;Cosine accuracy:&amp;quot;,(acc_cosine))
print(&amp;quot;Normalised-Euclidian:&amp;quot;,(acc_norm))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Euclidian accuracy: [94.33962264150944, 94.33962264150944, 96.22641509433963, 96.22641509433963]
Cosine accuracy: [92.45283018867924, 96.22641509433963, 98.11320754716981, 98.11320754716981]
Normalised-Euclidian: [96.22641509433963, 98.11320754716981, 98.11320754716981, 98.11320754716981]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k=[1,3,5,7]
sns.set(style=&amp;quot;whitegrid&amp;quot;)
ax=sns.barplot(x=k, y=acc_euc, data=iris)
ax.set(ylim=(85, 100))
ax.set(xlabel=&#39;K-value&#39;, ylabel=&#39;Accuracy(Euclidian)&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[Text(0, 0.5, &#39;Accuracy(Euclidian)&#39;), Text(0.5, 0, &#39;K-value&#39;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_28_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k=[1,3,5,7]
sns.set(style=&amp;quot;whitegrid&amp;quot;)
ax=sns.barplot(x=k, y=acc_cosine, data=iris)
ax.set(ylim=(85,100))
ax.set(xlabel=&#39;K-value&#39;, ylabel=&#39;Accuracy(Cosine)&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[Text(0, 0.5, &#39;Accuracy(Cosine)&#39;), Text(0.5, 0, &#39;K-value&#39;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_29_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k=[1,3,5,7]
sns.set(style=&amp;quot;whitegrid&amp;quot;)
ax=sns.barplot(x=k, y=acc_cosine, data=iris)
ax.set(ylim=(85,100))
ax.set(xlabel=&#39;K-value&#39;, ylabel=&#39;Accuracy(Normalized Euclidian)&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[Text(0, 0.5, &#39;Accuracy(Normalized Euclidian)&#39;), Text(0.5, 0, &#39;K-value&#39;)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_30_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=font-weight:bold;&gt;We see the various accuracies we get on euclidian , cosine and normalised euclidian distances with the various k values of 1,3,5,7.Let&#39;s see what happens when we run the fit and predict on hyperparameter K value from 1 to 20&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;acc_cosine1=[]
acc_euc1=[]
acc_norm1=[]
for k in range(1,20,1):
    pred_euc,pred_cosine,pred_norm = My_KNN_predict(X_dev, y_dev, X_test,k)
    # evaluating accuracy
    accuracy1 = accuracy_scor(y_test, pred_euc)
    acc_euc1.append(accuracy1)
    accuracy2=accuracy_scor(y_test, pred_cosine)
    acc_cosine1.append(accuracy2)
    accuracy3=accuracy_scor(y_test, pred_norm)
    acc_norm1.append(accuracy3)
print(&amp;quot;Euclidian accuracy:&amp;quot;,max(acc_euc1),acc_euc1.index(max(acc_euc1))+1)
print(&amp;quot;Cosine accuracy:&amp;quot;,max(acc_cosine1),acc_cosine1.index(max(acc_cosine1))+1)
print(&amp;quot;Normalised-Euclidian:&amp;quot;,max(acc_norm1),acc_norm1.index(max(acc_norm1))+1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Euclidian accuracy: 100.0 7
Cosine accuracy: 100.0 1
Normalised-Euclidian: 100.0 4
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=font-weight:bold;&gt; We can see that considering the given parameters we get optimal values at k=5 for cosine similarity and if we iterate over values of k on 1 to 20 we get perfect scores which maybe due to perfectly labelled data or sometimes overfitting.
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def My_KNN_predict_test(X_dev, y_dev, X_test, k):
    pred_cosine=[]
    for i in range(len(X_test)):
        pred_cosine.append(My_knn_fit(X_dev, y_dev, X_test[i, :], k,&amp;quot;cosine&amp;quot;))
    pred_cosine=np.array(pred_cosine)
    #print(&amp;quot;cos:&amp;quot;,pred_cosine)
    #print(&amp;quot;norm:&amp;quot;,pred_norm)
    return (pred_euc,pred_cosine,pred_norm)
print(&amp;quot;Final accuracy of the optimal hyperparameter wrt to test set:&amp;quot;,acc_cosine[2])
def My_knn_fit_test(X_dev, y_dev, x_test, k,measure):
        if(measure==&amp;quot;cosine&amp;quot;):
            dist_cos = []
            class_output = []

        for i in range(len(X_dev)):
            cos_sim = np.sum((dot(x_test,X_dev[i, :]))/(norm(X_dev[i, :])*norm(x_test)))
            cos_dis=(1-cos_sim)
            dist_cos.append([(cos_dis),i])
            
        dist_cos_sort = sorted(dist_cos)
        
        for i in range(k):
            class_in = dist_cos_sort[i][1]
            class_output.append(y_dev[class_in])
        majority_vote=Counter(class_output).most_common(1)[0][0]   
        return majority_vote
    
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Final accuracy of the optimal hyperparameter wrt to test set: 98.11320754716981
&lt;/code&gt;&lt;/pre&gt;
&lt;p  style=&#34;font-family:Georgia;font-size:16px;&#34;&gt; As we can already see , one of the most advantageous features of kNN apart from being simple and easy to understand is that requires minimal to no training time and serves as a good starting point in learning algorithms.We also saw that though &#34;COSINE SIMILARITY WITH A HYPERPARAMTER OF 5&#34; performed the best with an almost perfect accuracy , it took a lot of time to process as might have compared to other algorithms.Some of the improvements that can be done are using other distance metrics to measure perfomance or using dimensionality reduction techniques depending on the dataset.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>UNDERSTANDING OVERFITTING USING POLYNOMIAL REGRESSION</title>
      <link>https://sushantmhambrey.github.io/post/assn1/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://sushantmhambrey.github.io/post/assn1/</guid>
      <description>&lt;p&gt;We know that even though linear models can provide good training models rudimentarily, there are lots of situations where the variables don&amp;rsquo;t reveal a linear relationship. Thus we need to create polynomial models for such datasets.One major issue with polynomial models is that they are suspectible to overfitting.In this article we will look at how a higher degree polynomial model overfits a dataset to create a perfect training environment as opposed to it&amp;rsquo;s errors introduced while testing on the same degree.We will further look at how regularization helps to tackel overfitting and what values of hyperparameters produce the best results.&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;Importing the necessary libraries&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math
import pandas as pd
import operator
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
#from sklearn.linear_model import Lasso
#from sklearn.linear_model import ElasticNet
from sklearn.pipeline import make_pipeline
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
&lt;/code&gt;&lt;/pre&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt;
Creating X,Y data pairs.Here x is sampled from a uniform distribution and N from a gaussian normal distribution&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#we can change the seed value to get different randome numbers for our x,N values
np.random.seed(45)
x=np.random.uniform(low=0,high=1,size=20)
mu=0
sigma=1
#N is based on a gaussian normal distribution
N=np.random.normal(mu,sigma,size=20)
y=(np.sin(2*np.pi*x))+N
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#splitting the dataset into 10 for training and 10 for testing
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)
x_train
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0.97600332, 0.62739168, 0.44053089, 0.99072168, 0.16332445,
       0.07728957, 0.28266721, 0.673068  , 0.47280797, 0.048522  ])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#a reference true fit graph to see how our data fits with different polynomial degrees
x_rn=np.linspace(0,1,100)
y_rn=(np.sin(2*np.pi*x_rn))
plt.scatter(x_train,y_train,s=10,color=&amp;quot;red&amp;quot;)
plt.plot(x_rn,y_rn)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_7_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; 
 Making the graphs for fit data with the specific degrees ranging from 0 to 9 &lt;/p&gt;
 &lt;p style=font-weight:bold;&gt; We can observe that as we increase the degree of our polynomial regression model , the graph tends to cover all the datapoints leading to overfitting &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.pipeline import make_pipeline
#x = np.sort(x_train[:])
#y=np.sort(y_train[:])
train_error=[]
test_error=[] 
for i in range(10):
    plt.title(&#39;Degree %d&#39; %i)
    plt.text(0.7, .55, &#39;M=%d&#39; %i)
    X = x_train[:, np.newaxis]
    Y = y_train[:, np.newaxis]
    X1 = x_test[:,np.newaxis]
    Y1 = y_test[:,np.newaxis]
    #we first make use of the linearregression model to observe how it overfits at higher degrees.
    model = make_pipeline(PolynomialFeatures(i), LinearRegression())

    model.fit(X,Y)
    y_pred = model.predict(X)
    mse = (mean_squared_error(Y,y_pred))
    rmse=math.sqrt(mse)
    train_error.append(rmse)
    
    y_test_pred=model.predict(X1)
    mse_test= (mean_squared_error(Y1,y_test_pred))
    rmse_test=math.sqrt(mse_test)
    test_error.append(rmse_test)
    #sorting
    lists=sorted(zip(*[X,y_pred]))
    X,y_pred = list(zip(*lists))
    #plotting the models at various degrees.
    plt.scatter(x_train, y_train,color=&#39;black&#39;,label=&#39;data points&#39;)
    plt.plot(X, y_pred, color=&#39;g&#39;,label=&#39;degree_fit&#39;)
    plt.plot(x_rn,y_rn,color=&#39;r&#39;,label=&#39;true_fit&#39;)
    plt.legend(loc=&amp;quot;lower left&amp;quot;)
    _=plt.xlabel(&amp;quot;X--&amp;gt;&amp;quot;)
    _=plt.ylabel(&amp;quot;t--&amp;gt;&amp;quot;)
    plt.show()
    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_3.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_4.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_5.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_6.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_7.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_8.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_9_9.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; 
    Plotting training vs test error
    &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.plot(train_error,label=&#39;train error&#39;)
plt.plot(test_error,label=&#39;test error&#39;)
plt.xticks(np.arange(0, 10, 1.0))
plt.legend(loc=&amp;quot;upper left&amp;quot;)
_=plt.xlabel(&amp;quot;M&amp;quot;)
_=plt.ylabel(&amp;quot;E(RMS)&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_11_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; 
    Generating 100 data points and fitting ninth order model on it &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#creating 100 data points
np.random.seed(10)
x2=np.random.uniform(low=0,high=1,size=100)
mu=0
sigma=1
N2=np.random.normal(mu,sigma,size=100)
y2=(np.sin(2*np.pi*x2))+N2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_train100, x_test100, y_train100, y_test100 = train_test_split(x2, y2, test_size=0.01)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X100 = x_train100[:, np.newaxis]
Y100 = y_train100[:, np.newaxis]
model = make_pipeline(PolynomialFeatures(9), LinearRegression())

#fitting the 100 data points of the ninth order model.
model.fit(X100,Y100)
y_pred100 = model.predict(X100)

 #sorting
lists=sorted(zip(*[X100,y_pred100]))
X100,y_pred100 = list(zip(*lists))
    
plt.scatter(x_train100, y_train100,color=&#39;#3299a8&#39;,label=&#39;data points&#39;)
plt.plot(X100, y_pred100, color=&#39;r&#39;,label=&#39;model fit&#39;)
plt.plot(x_rn,y_rn,color=&#39;b&#39;,label=&#39;true fit&#39;)
plt.legend(loc=&amp;quot;lower left&amp;quot;)
_=plt.xlabel(&amp;quot;X--&amp;gt;&amp;quot;)
_=plt.ylabel(&amp;quot;Y--&amp;gt;&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_15_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; Regularisation and graph creation for different values of lambda&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lbd=[1, 1/10, 1/100, 1/1000, 1/10000, 1/100000]
plt.ylim(-2,2)
train_error2=[]
test_error2=[]
for i in range(6):
    model1= make_pipeline(StandardScaler(),PolynomialFeatures(degree=9), Ridge(alpha=lbd[i],fit_intercept=True))
    
    model1.fit(X,Y)
    y_pred2 = model1.predict(X)
    mse2 = (mean_squared_error(Y,y_pred2))
    rmse2=math.sqrt(mse2)
    train_error2.append(rmse2)
    
    y_test_pred2=model1.predict(X1)
    mse_test2= (mean_squared_error(Y1,y_test_pred2))
    rmse_test2=math.sqrt(mse_test2)
    test_error2.append(rmse_test2)
    
    lists=sorted(zip(*[X,y_pred2]))
    X,y_pred2 = list(zip(*lists))
    
    plt.scatter(X,Y,color=&#39;black&#39;)
    plt.plot(X, y_pred2, color=&#39;g&#39;)
    plt.plot(x_rn,y_rn,color=&#39;r&#39;)
    _=plt.xlabel(&amp;quot;X--&amp;gt;&amp;quot;)
    _=plt.ylabel(&amp;quot;t--&amp;gt;&amp;quot;)
    plt.show()
    #print(&#39;Score: {}&#39;.format(model1.score(X,Y)))
    #print(&#39;Test :{}&#39; .format(model1.score(x_test.reshape(-1,1),y_test.reshape(-1,1))))
    
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_17_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_17_1.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_17_2.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_17_3.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_17_4.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./index_17_5.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=background-color:#6aa2de;font-weight:bold;font-size:15px;text-align:center;&gt; Plotting Training vs Test values for various lambda &lt;/p&gt; 
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.plot(np.log(lbd),train_error2,label=&#39;train error&#39;)
plt.plot(np.log(lbd),test_error2,label=&#39;test error&#39;)
#plt.xscale(&amp;quot;log&amp;quot;)
#plt.xticks(np.arange(0, 1.1, 0.1))
plt.xlim(-10,0)
plt.legend(loc=&amp;quot;upper right&amp;quot;)
_=plt.xlabel(&amp;quot;ln(lambda)&amp;quot;)
_=plt.ylabel(&amp;quot;E(RMS)&amp;quot;)
plt.show()
#print(test_error2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./index_19_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt;
&lt;p style=&#34;font-family:Georgia;font-size:16px;background-color:#2455d1;color:white;&#34;&gt; Based on best test perfomance the Ridge model helps us to regularize our overfitting which we could not do throug LinearRegression. We observe that as the model complexity increases,bias decreases and variance increases and vice versa.Also,we can see that as the lambda value decreases we get a good training score and we obtain the best training score for lambda =1/100000.But thats not the case with the testing score.Before we regularise, we see that the model of degree 9 fits all the data points but leads to overfitting.So a polynoial regression model of degree 6 will be best amongst what we tried without leading to overfit.Also after we regularize we see that the lambda values are almost constant before increasing so the model with lambda 1/1000 seems to be the best &lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
